\documentclass{svproc}
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{biblatex}
\addbibresource{lit.bib}

\title{Exploring the effectiveness of Small-Scale Vision and Language Models for Vision and Language Navigation Tasks in Continuous Environments}
\author{Wesley Chiu, Abdulrahman Altahhan}
\institute{University of Leeds, School of Computing, ODL MSc in AI, UK.}
\date{January 2025}

\begin{document}
\maketitle

\begin{abstract}
    Vision-and-Language Navigation (VLN) is a rapidly evolving field of research that aims to enable an embodied agent to follow textual instructions given in natural language to navigate through an unseen environment to a goal position. Existing approaches to this ask all into two main categories: "specialist" models that have been constructed and trained specifically to solve this task, and zero-shot or few-shot models that aim to leverage the implicit knowledge within Large Language Models (LLMs) and Vision Language Models  (VLMs). Prior work in the latter approach have used the most powerful models (70B+ parameters). This work aims to evaluate the suitability of using a lighter-weight variant of an existing open-sourced model (Qwen2-VL) as the primary VLM, which would allow it to be run "on-device" rather than being connected to a broader network. The experiments in a simulated environment demonstrates that the current ability of the lighter-weight model is not yet fit for purpose, failing to reach the success rates 
    seen approaches that use the full-sized variants.
    
    \keywords{Vision-and-Language Navigation, Vision Language Model, Large Language Model, Prompt Engineering, Prompting}
\end{abstract}

\section{Introduction}
    Vision and Language Navigation (VLN) is a rapidly evolving field of research, which tasks an embodied agent to follow a set of textual instructions given in natural language to navigate a previously unseen 3D indoor environment. Whilst initial research leveraged models such as RNNs and CNNs to process visual and natural language data, the introduction of Transformer-based models \cite{attenion_is_all_you_need} has accelerated capabilities in Natural Language Processing (NLP) and Computer Vision (CV) - both key components in the VLN task. The rapid development of Large Language Models (LLMs) and Vision-Language Models (VLMs) has further intensified the volume and pace of research in VLN, with each advance in LLM and VLM competency directly benefiting the VLN task.
    The traditional VLN task, as proposed by Andersen, et al. (2018) in \cite{8578485}, has the agent navigate between pre-defined nodes in the environment (sometimes referred to as a navigation graph), which essentially reduces the VLN task to a vision-based graph-search problem; models trained in this manner have difficulty translating to performance in the real-world \cite{pmlr-v155-anderson21a}. To help combat this gap, the VLN Continuous Environment (VLN-CE) task was introduced, and has no such predefined navigation points, allowing agents to take any action to reach a navigable point in the environment. This introduces new challenges for the models to overcome, such as avoiding getting stuck on obstacles and dealing with distances \cite{krantz2020navgraphvisionandlanguagenavigationcontinuous}.
    \newline \par
    Approaches to the VLN task (whether graph-based VLN or VLN-CE) that leverage existing LLMs and VLMs typically fall into two broad categories: 'generalist' models or 'specialist' models.
    \par
    Generalist models \cite{long2023_discussnav, chen2024mapgptmapguidedpromptingadaptive, zhou2023navgptexplicitreasoningvisionandlanguage} use 'off-the-shelf' models such as OpenAI's ChatGPT \cite{CITATION_REQUIRED} and aim to leverage the implicit knowledge, NLP ability, and strong reasoning skills within LLMs and VLMs \cite{51647, LMs_as_knowledge_bases} to navigate the environment. These approaches rely on specific prompting to generate historical trajectories, make navigation decisions, and to monitor progress.
    \par
    Specialist models \cite{hong2021_vlnbert, navgpt2, chen2021_HAMT, HE2024110511_MemoryAdaptiveVLN} are built from the ground-up, typically with specialist sub-models, to tackle the VLN task. These approaches exhibit stronger performance in VLN tasks compared to the generalist approach, but are less unable to take advantage of more competent LLMs or VLMs as they are released in the same 'plug-and-play' manner of generalist models.
    \newline
    Both approaches typically leverage larger, complex language and vision models, requiring powerful hardware with high amounts of memory to run during inferencing. The development of Low Rank Adaptation (LoRA) finetuning \cite{LoRA} combined with model quantization allows for a LLM or VLM to be trained and run on hardware with more limited memory. This is particularly relevant to the VLN task, as lighter-weight models can be run more readily on smaller robots with more limited hardware, broadening the application of these models.
    \newline
    This work explores the effectiveness of using a small, finetuned, quantized open-sourced model as the primary VLM to tackle the VLN-CE task and examines its performance against other generalist approaches. 
    
\section{Literature Review}
    \textbf{Vision and Language Navigation (VLN)}  The ability for an embodied agent to navigate a previously unseen environment purely by natural language instruction is a field of research that has seen increasing interest since the task was formally introduced by Andersen et al. in 2017 \cite{8578485}. The prospect of a generalizable model that could be transported to different environments without additional training has a broad range of applications for robotics.
    \newline
    The introduction of Large Language Models (LLMs) has accelerated research into the VLN task, with LLMs being used not only in interpreting the natural language instructions, but also in decision making and other core components of VLN models.
    \newline \par
    \textbf{Large Language Models and Vision Models in VLN}  ... (talk about LangNav and others) (use of GPT) (generalised vs specialist models) (Open-Nav uses Qwen2-72B?)
    \newline \par
    \textbf{Challenges in Continuous Environments} ... (talk about Sim2Real challenges) (speak about work that used node-based navigation)
    \newline
    However, these models typically rely on translating visual data into purely textual information, which risks a loss of information in the process \cite{pan2024langnavlanguageperceptualrepresentation}; as a result, these models typically underperform against specialist models. The strength of these models lie in the fact that no pre-training is required, and as LLMs and VLMs continue to improve, this increase in competency can directly translate into improved VLN performance without the need for pre-training or adaptation of a complex model structure.
    \newline
    \newline


\section{Methodology}
    The approach taken in this work, as described in \ref{fig:fig2}, is to use the open-sourced VLM Qwen2-VL-7B-Instruct \cite{qwen2} as the primary component of the model to help the model understand where it located in the environment, track its historical trajectory, and to make decisions on its next action. A small component of this was architecture leverages OpenAI's GPT4-mini model to break down the natural language instructions into discrete "Navigation Checkpoints" (why?). Finally, the agent was placed in a simulated environment and asked to follow the Navigation Checkpoints to its goal.
    
    \textbf{Datasets and Simulation}  Matterport3D \cite{Matterport3D}, Habitat-Sim \cite{habitat19iccv, szot2021habitat, puig2023habitat3}, RxR \cite{rxr}.

    \textbf{Data Generation for Continuous Environment}  The ground-truth trajectory provided by the RxR dataset was created based on a node-based navigation system, which allows an agent to move from one node to another without considering navigational obstacles between nodes. This is insufficient for finetuning the model for use in a continuous environment and has proven to be one of the obstacles in translating graph-based models into real world environments \cite{pmlr-v155-anderson21a}. In order to generate training data to finetune the VLM, 

    

    \begin{figure}
        \centering
        \includegraphics[scale=.75]{figures/DecisionBoundary.png}
        \caption{Caption}
        \label{fig:fig2}
    \end{figure}

    
    \begin{figure}
        \centering
        \includegraphics[scale=.75]{figures/DecisionBoundary.png}
        \caption{Caption}
        \label{fig:my_label}
    \end{figure}


    
    In Fig. \ref{fig:my_label} that $\alpha = n^2$. In eq. (\ref{eq:sum_i}) we have shown that the $ 1+2+3+4 = 4\times 5 /2=10$.
    \begin{align*}
        \sum_{i=1}^{n} y &= 10 \\
        M &= \beta ^2 \\
        \boldsymbol{B}^\top &= \boldsymbol{\Lambda}^2 \\
    \end{align*}
    
    \begin{align}
        \label{eq:sum_i}
        \sum_{i=1}^n i &= \frac{(n+1)n}{2} \\  
        \nonumber
        \beta ^2 &= \alpha_i^n
    \end{align}

\section{Experiment Results}
    \subsection{Experiment 1}
        \subsubsection{Exp}
    Note that the figure and the tables might be laid out on another page. Do not worry about that, and do not attempt to change it. Leave this to Latex.
    
    
    \begin{table}
        \caption{This is a table}
        \begin{center}
            \begin{tabular}{rlc}
                \hline
                \multicolumn{1}{l}{Year}&\multicolumn{1}{l}{World}&\multicolumn{1}{l}{Duration}\\
                \hline
                8000 B.C.  &     5,000,000 &  10\\
                  50 A.D.  &   200,000,000 &  20\\
                1650 A.D.  &   500,000,000 &  30\\
                \hline
            \end{tabular}
        \end{center}
    \end{table}

\section{Conclusion and Future Work}
We have conducted a study on ...

\printbibliography 
\end{document}
