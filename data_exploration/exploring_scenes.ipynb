{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6902ac7-d79c-4564-80f7-17ba4aedf2bb",
   "metadata": {},
   "source": [
    "# To-Do next:\n",
    "1. ~~Make `scene_path` update using `get_scene_path`~~\n",
    "2. ~~Update `scene` in `sim_settings` to use the new `scene_path`~~\n",
    "3. ~~Fix the positioning of agent on the map (done?)~~\n",
    "4. ~~Create a video showing the agent spinning around to demonstrate environment~~\n",
    "5. Check to see if there is a better way to account for agent height (?)\n",
    "6. ~~Work out 360 view~~ _(Assuming that this is not required for now)_\n",
    "7. ~~Work out prompt for agent to describe task~~\n",
    "8. ~~Hook up a ViT~~\n",
    "9. Functionise everything\n",
    "10. Make agent take a series of steps\n",
    "11. Plot trajectory on a map\n",
    "\n",
    "# Major milestones\n",
    "1. ~~Map agent in environment~~\n",
    "2. Agent random-walk with position trace\n",
    "3. Work out 360 view\n",
    "4. ~~Hook up vision transformer~~\n",
    "5. Hook up LLM\n",
    "6. Hook up second vision transformer\n",
    "7. ??\n",
    "8. Write-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ad8b7-7581-4ba3-abc1-766bf4857e0f",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "Setting up:\n",
    " - Required modules\n",
    " - Folder paths\n",
    " - Useful functions\n",
    " - Creating simulator instance and reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da0df8e-8d08-49b4-9ec4-3d09822df556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PluginManager::Manager: duplicate static plugin StbImageImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin GltfImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin BasisImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin AssimpImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin AnySceneImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin AnyImageImporter, ignoring\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import imageio\n",
    "import magnum as mn\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import habitat_sim\n",
    "from habitat_sim.utils import common as utils\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "from habitat.utils.visualizations import maps\n",
    "import magnum as mn\n",
    "\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e508ac6c-f689-4888-942e-6a230bbaa51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up folder paths\n",
    "habitat_dir_path = \"/home/wes/habitat/data/\"\n",
    "rxr_data_path = \"/media/wes/Lexar/rxr_data/rxr-data/\"\n",
    "output_dir_path = \"/home/wes/Documents/git_ws/OCOM5300M_ResearchProject/outputs/\"\n",
    "\n",
    "scene_dir_path = os.path.join(habitat_dir_path, f\"scene_datasets/mp3d/\")\n",
    "pose_dir_path = os.path.join(rxr_data_path, f'pose_traces/rxr_train/')\n",
    "mp3d_scene_config = os.path.join(scene_dir_path, f\"mp3d.scene_dataset_config.json\")\n",
    "\n",
    "scene_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d749e10d-0c13-42fa-9a91-d4cc12bce250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True \t scene_dir_path: /home/wes/habitat/data/scene_datasets/mp3d/\n",
      "Exists: True \t pose_dir_path: /media/wes/Lexar/rxr_data/rxr-data/pose_traces/rxr_train/\n",
      "Exists: True \t mp3d_scene_config: /home/wes/habitat/data/scene_datasets/mp3d/mp3d.scene_dataset_config.json\n"
     ]
    }
   ],
   "source": [
    "# checking that paths exist\n",
    "print(f'Exists: {os.path.exists(scene_dir_path)} \\t scene_dir_path: {scene_dir_path}')\n",
    "print(f'Exists: {os.path.exists(pose_dir_path)} \\t pose_dir_path: {pose_dir_path}')\n",
    "print(f\"Exists: {os.path.exists(mp3d_scene_config)} \\t mp3d_scene_config: {mp3d_scene_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1412c8-7272-4ddb-9fb4-300a87c22683",
   "metadata": {},
   "source": [
    "## Preamble: Utility Functions\n",
    "### Path updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6118f18c-3cee-4f37-9be5-ba4b1dc21c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the full filepath of a scene given a scene id (i.e. scan id)\n",
    "def get_scene_path(scene_id):\n",
    "    global scene_dir_path\n",
    "    return os.path.join(scene_dir_path, f\"{scene_id}/{scene_id}.glb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd701717-c2f8-4ffd-afa8-016489c1229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate the new scene filepath across all relevant dictionaries, etc.\n",
    "def update_scene_path(scene_id):\n",
    "    global scene_dir_path\n",
    "    scene_path = get_scene_path(scene_id)\n",
    "\n",
    "    return scene_id, scene_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ce0c9-b159-40c3-841c-ac884d42c080",
   "metadata": {},
   "source": [
    "### Display Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d6ec995-cb0f-4f73-be60-288d87c61fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to do something like this maybe: https://stackoverflow.com/a/41432704\n",
    "#    i.e. show multiple subplots on one plot\n",
    "def display_sample(rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([])):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [\"rgb\", \"depth\"]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 2, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b10467-3597-4957-af01-020422dd93ef",
   "metadata": {},
   "source": [
    "### Configuring Simulator Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a1f834-b92d-4cb8-8861-3fe93ea19209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default sim settings\n",
    "rgb_sensor = True\n",
    "depth_sensor = True\n",
    "semantic_sensor = False\n",
    "seed = 1\n",
    "number_images_for_panorama = 4 # number of images required for a full 360 degree panorama to be created\n",
    "\n",
    "\n",
    "sim_settings = {\n",
    "    \"width\": 256,  # Spatial resolution of the observations\n",
    "    \"height\": 256,\n",
    "    \"scene\": scene_path,  # Scene path\n",
    "    \"scene_dataset\": mp3d_scene_config,  # the scene dataset configuration files\n",
    "    \"default_agent\": 0,\n",
    "    \"sensor_height\": 1.0,  # Height of sensors in meters\n",
    "    \"color_sensor\": rgb_sensor,  # RGB sensor\n",
    "    \"depth_sensor\": depth_sensor,  # Depth sensor\n",
    "    \"semantic_sensor\": semantic_sensor,  # Semantic sensor\n",
    "    \"seed\": seed,  # used in the random navigation\n",
    "    \"enable_physics\": False,  # kinematics only    \n",
    "    \"hfov\" : 360 / number_images_for_panorama\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2168b9-4791-46a7-938a-01db7187f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sim config factory?\n",
    "def make_cfg(settings):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.scene_id = settings[\"scene\"]\n",
    "    sim_cfg.scene_dataset_config_file = settings[\"scene_dataset\"]\n",
    "    sim_cfg.enable_physics = settings[\"enable_physics\"]\n",
    "\n",
    "    sensor_specs = []\n",
    "\n",
    "    # RGB sensor specs\n",
    "    color_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    color_sensor_spec.uuid = \"color_sensor\"\n",
    "    color_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    color_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    color_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    color_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    color_sensor_spec.hfov = settings[\"hfov\"]\n",
    "    sensor_specs.append(color_sensor_spec)\n",
    "\n",
    "    # Depth sensor specs\n",
    "    depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    depth_sensor_spec.uuid = \"depth_sensor\"\n",
    "    depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "    depth_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    depth_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    depth_sensor_spec.hfov = settings[\"hfov\"]\n",
    "    sensor_specs.append(depth_sensor_spec)\n",
    "\n",
    "    #agent movement\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "    agent_cfg.action_space = {\n",
    "        \"move_forward\" : habitat_sim.agent.ActionSpec(\n",
    "            \"move_forward\", habitat_sim.agent.ActuationSpec(amount=0.25)\n",
    "        ),\n",
    "        \"turn_left\":habitat_sim.agent.ActionSpec(\n",
    "            \"turn_left\", habitat_sim.agent.ActuationSpec(amount=30.0)\n",
    "        ),\n",
    "        \"turn_right\":habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=30.0)\n",
    "        )\n",
    "    }\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd40cb6-7492-4f23-aa5b-165a97a6764b",
   "metadata": {},
   "source": [
    "### Utility functions to map agent positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00873423-eeab-45b9-a290-8af06fd59c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 3d points to 2d topdown coordinates\n",
    "def convert_points_to_topdown(pathfinder, points, meters_per_pixel):\n",
    "    points_topdown = []\n",
    "    bounds = pathfinder.get_bounds()\n",
    "    for point in points:\n",
    "        # convert 3D x,z to topdown x,y\n",
    "        px = (point[0] - bounds[0][0]) / meters_per_pixel\n",
    "        py = (point[2] - bounds[0][2]) / meters_per_pixel\n",
    "        points_topdown.append(np.array([px, py]))\n",
    "    return points_topdown\n",
    "\n",
    "\n",
    "# display a topdown map with matplotlib\n",
    "def display_map(topdown_map, key_points=None):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    #ax.axis(\"off\")\n",
    "    #ax.set_xlim([-30, 30])\n",
    "    #ax.set_ylim([-30,30])\n",
    "    plt.imshow(topdown_map)\n",
    "    # plot points on map\n",
    "    if key_points is not None:\n",
    "        for point in key_points:\n",
    "            plt.plot(point[0], point[1], marker=\"o\", markersize=10, alpha=0.8)\n",
    "            print(f'Adding point to {point[0]}, {point[1]}')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925119b-6489-4210-9be1-d47d5b81cc54",
   "metadata": {},
   "source": [
    "### Loading annotations file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a0e16d-08ae-4069-b259-b7a50cb9003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read rxr data and return it as a dataframe\n",
    "def json_to_df(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return pd.DataFrame(data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cb0fe-ebbf-4df1-92aa-faea63c23482",
   "metadata": {},
   "source": [
    "### Translating `rxr` viewpoint IDs into `habitat-sim` positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "086557b5-22f6-4aa2-94fc-53a848d4a30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>row1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      col1  col2  col3\n",
       "row1     1  0.50   3.0\n",
       "row2     2  0.75   4.0\n",
       "row3     3  0.60   0.2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'col1': [1, 2, 3],\n",
    "                   'col2': [0.5, 0.75, 0.6],\n",
    "                   'col3': [3.0, 4.0, 0.2]},\n",
    "                  index=['row1', 'row2', 'row3'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ab9c8b1-791d-4d38-9a8c-8d224d92388a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}, {'col1': 3, 'col2': 0.6}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['col1', 'col2']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba2d3b22-3e51-4ff9-9f0d-498dd2bcdd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6149937a-2edb-43a5-835b-10d5d4537bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a scan id (aka: scene_id) and annotation file path\n",
    "# Returns a list of scene_ids and associated instruction_ids and instructions\n",
    "# If scan_id is provided, then only annotations and instruction ids for the scene_id is returned\n",
    "def get_rxr_annotations(annotations_path, scan_id_list = 'None'):\n",
    "    if not os.path.exists(annotations_path):\n",
    "        print(f'Annotation path not found: {annotations_path}')\n",
    "        return\n",
    "\n",
    "    n_records = 0\n",
    "    rxr_annotations = {}\n",
    "    # Turning entire annotation json into dataframe\n",
    "    df_rxr = json_to_df(annotations_path)\n",
    "    \n",
    "    # Focusing on just the english instructions for now\n",
    "    df_rxr = df_rxr[df_rxr['language']=='en-US']\n",
    "\n",
    "    # If no scan_id provided, go through entire dataset\n",
    "    if scan_id_list == 'None':\n",
    "        scan_id_list = df_rxr['scan'].unique().tolist()\n",
    "\n",
    "    # Return a dictionary\n",
    "    # key = scan_id\n",
    "    # value = [{instruction_id}:{instructions}, ...]\n",
    "    for scan_id in scan_id_list:\n",
    "        rxr_annotations[scan_id] = df_rxr[df_rxr['scan']==scan_id][['instruction_id', 'instruction']].to_dict('records')\n",
    "        n_records += len(rxr_annotations[scan_id])\n",
    "\n",
    "    return scan_id_list, rxr_annotations, n_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddcfedae-1a47-469e-a2d2-f5ceb7e60f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_fname = \"rxr_train_guide.jsonl/rxr_train_guide.json\"\n",
    "annotations_path = os.path.join(rxr_data_path, annotations_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa0400e2-995a-4fcd-b0b7-c0939eb91fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id_list, rxr_annotations, n_records = get_rxr_annotations(annotations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29412fdb-6990-4902-a55d-6653576c7bf6",
   "metadata": {},
   "source": [
    "## Reading in the `rxr` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d28d088-75c3-4c66-8b8b-060ff1d21401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking existence of /media/wes/Lexar/rxr_data/rxr-data/rxr_train_guide.jsonl/rxr_train_guide.json: \t True\n"
     ]
    }
   ],
   "source": [
    "# Checking file path\n",
    "annotations_fname = \"rxr_train_guide.jsonl/rxr_train_guide.json\"\n",
    "annotations_path = os.path.join(rxr_data_path, annotations_fname)\n",
    "print(f'Checking existence of {annotations_path}: \\t {os.path.exists(annotations_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61360198-96ee-4abd-b0a3-6b25c44b3aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_id</th>\n",
       "      <th>split</th>\n",
       "      <th>scan</th>\n",
       "      <th>heading</th>\n",
       "      <th>path</th>\n",
       "      <th>instruction_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>language</th>\n",
       "      <th>instruction</th>\n",
       "      <th>timed_instruction</th>\n",
       "      <th>edit_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>SN83YJsR3w2</td>\n",
       "      <td>2.631565</td>\n",
       "      <td>[4471fcf26b3847ed88ce41eca5ecb13d, b2b0b597ef2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en-US</td>\n",
       "      <td>You will start by standing in front of a glass...</td>\n",
       "      <td>[{'end_time': 1.5, 'word': 'You', 'start_time'...</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>7y3sRwLe3Va</td>\n",
       "      <td>5.875003</td>\n",
       "      <td>[60ce99b0264148c09db7ef836ad77e3f, dd83fb40a2e...</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>en-US</td>\n",
       "      <td>You're starting in a closet, facing empty shel...</td>\n",
       "      <td>[{'start_time': 3.5, 'end_time': 3.8, 'word': ...</td>\n",
       "      <td>0.061920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>24</td>\n",
       "      <td>train</td>\n",
       "      <td>S9hNv5qa7GM</td>\n",
       "      <td>4.879265</td>\n",
       "      <td>[65683a5cc3564769947019eb7e63ef0d, bd9faec23bb...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>en-US</td>\n",
       "      <td>You begin facing a wall, turn to your left, ta...</td>\n",
       "      <td>[{'word': 'You', 'start_time': 1.0, 'end_time'...</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>27</td>\n",
       "      <td>train</td>\n",
       "      <td>ur6pFq6Qu1A</td>\n",
       "      <td>2.913781</td>\n",
       "      <td>[23b6df4fc549460daab1c23d3674f845, 96b2d4806ff...</td>\n",
       "      <td>67</td>\n",
       "      <td>86</td>\n",
       "      <td>en-US</td>\n",
       "      <td>You're looking at a Japanese decorative plate ...</td>\n",
       "      <td>[{'start_time': 2.5, 'end_time': 3.0, 'word': ...</td>\n",
       "      <td>0.137037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>29</td>\n",
       "      <td>train</td>\n",
       "      <td>mJXqzFtmKg4</td>\n",
       "      <td>1.764716</td>\n",
       "      <td>[b7dd551438384d4da9530d3815d3aa90, 50d87f990a7...</td>\n",
       "      <td>71</td>\n",
       "      <td>81</td>\n",
       "      <td>en-US</td>\n",
       "      <td>You begin facing a bunch of cabinets, moving s...</td>\n",
       "      <td>[{'start_time': 1.4, 'word': 'You', 'end_time'...</td>\n",
       "      <td>0.070600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    path_id  split         scan   heading  \\\n",
       "0         0  train  SN83YJsR3w2  2.631565   \n",
       "21       12  train  7y3sRwLe3Va  5.875003   \n",
       "47       24  train  S9hNv5qa7GM  4.879265   \n",
       "54       27  train  ur6pFq6Qu1A  2.913781   \n",
       "55       29  train  mJXqzFtmKg4  1.764716   \n",
       "\n",
       "                                                 path  instruction_id  \\\n",
       "0   [4471fcf26b3847ed88ce41eca5ecb13d, b2b0b597ef2...               0   \n",
       "21  [60ce99b0264148c09db7ef836ad77e3f, dd83fb40a2e...              28   \n",
       "47  [65683a5cc3564769947019eb7e63ef0d, bd9faec23bb...              60   \n",
       "54  [23b6df4fc549460daab1c23d3674f845, 96b2d4806ff...              67   \n",
       "55  [b7dd551438384d4da9530d3815d3aa90, 50d87f990a7...              71   \n",
       "\n",
       "    annotator_id language                                        instruction  \\\n",
       "0              0    en-US  You will start by standing in front of a glass...   \n",
       "21            44    en-US  You're starting in a closet, facing empty shel...   \n",
       "47            81    en-US  You begin facing a wall, turn to your left, ta...   \n",
       "54            86    en-US  You're looking at a Japanese decorative plate ...   \n",
       "55            81    en-US  You begin facing a bunch of cabinets, moving s...   \n",
       "\n",
       "                                    timed_instruction  edit_distance  \n",
       "0   [{'end_time': 1.5, 'word': 'You', 'start_time'...       0.076923  \n",
       "21  [{'start_time': 3.5, 'end_time': 3.8, 'word': ...       0.061920  \n",
       "47  [{'word': 'You', 'start_time': 1.0, 'end_time'...       0.041667  \n",
       "54  [{'start_time': 2.5, 'end_time': 3.0, 'word': ...       0.137037  \n",
       "55  [{'start_time': 1.4, 'word': 'You', 'end_time'...       0.070600  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rxr = json_to_df(annotations_path)\n",
    "\n",
    "# only interested in the English instructions at this time\n",
    "df_rxr = df_rxr[df_rxr['language']=='en-US']\n",
    "df_rxr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d0a6c-158c-4e05-83b7-1cf1e18ea470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a scan for testing purposes\n",
    "scene_id, scene_path = update_scene_path(df_rxr['scan'].iloc[1])\n",
    "sim_settings['scene'] = scene_path\n",
    "\n",
    "# finding all instructions relating to that scene\n",
    "df_instruction = df_rxr[df_rxr['scan']==scene_id]\n",
    "df_instruction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ff436-ba90-4453-92d4-31b8eb7d8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the pose traces relating to that particular instruction\n",
    "instruction_id = df_instruction['instruction_id'].iloc[0]\n",
    "instruct_id = ('000000' + str(instruction_id))[-6:]\n",
    "print(f'pose_dir_path: {pose_dir_path}, \\t exists: {os.path.exists(pose_dir_path)}')\n",
    "print(f'instruct_id: {instruct_id}')\n",
    "print(f'Instructions: {df_instruction[\"instruction\"].iloc[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d029565-fb54-46b0-b3db-f6304da5438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the pose trace\n",
    "pose_trace_path = os.path.join(pose_dir_path, f'{instruct_id}_guide_pose_trace.npz')\n",
    "print(f'{pose_trace_path} exists: {os.path.exists(pose_trace_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22ef2d-a3c4-4feb-b29d-8ed8154f76b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_trace = np.load(pose_trace_path)\n",
    "poses = pose_trace['extrinsic_matrix']\n",
    "print(poses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff7535-a871-400e-91a0-464d0c1b1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_pano = pose_trace['pano']\n",
    "poses_pano[0] == df_instruction['path'].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ed040-ebb7-4cb5-9c42-74f3dc0fdf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of unique positions and rotations\n",
    "\n",
    "# Creating np array with proper dimensions\n",
    "unique_position_list = np.zeros((1,3))\n",
    "unique_rotation_list = np.zeros((1,3,3))\n",
    "for pose in poses:\n",
    "    if not np.all(np.isin(pose[:3,3], unique_position_list)):\n",
    "        unique_position_list = np.append(unique_position_list, np.array(pose[:3,3]).reshape(1,3), axis=0)\n",
    "        unique_rotation_list = np.append(unique_rotation_list, np.array(pose[:3,:3]).reshape(1,3,3), axis=0)\n",
    "\n",
    "# Removing the first position (full of zeros)\n",
    "unique_position_list = unique_position_list[1:]\n",
    "unique_rotation_list = unique_rotation_list[1:]\n",
    "\n",
    "print(f'Number of unique positions in the trace: {len(unique_position_list)}')\n",
    "print(f'Sample list of positions -----')\n",
    "print(unique_position_list[:3])\n",
    "print(f'\\nSample list of rotations -----')\n",
    "print(unique_rotation_list[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ffaf4-5101-4a36-b9bb-9b243403a1b0",
   "metadata": {},
   "source": [
    "## Creating simulator instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27ad87-9ead-402f-b603-46c8d2ed7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "cfg = make_cfg(sim_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94759a04-4251-4bfd-9bd5-f74514cced89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulator instance\n",
    "sim = habitat_sim.Simulator(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e36c7-edef-408c-bff8-87d372ed6be6",
   "metadata": {},
   "source": [
    "# Test - turning viewpoint IDs into positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97e6cb-37d0-421b-89e3-4247ffdf9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing instruction\n",
    "nav_instruction = df_instruction[df_instruction['instruction_id']==instruction_id]['instruction'].iloc[0]\n",
    "nav_instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fcd66-8820-4635-a728-f18268f1e3cc",
   "metadata": {},
   "source": [
    "## Snapping agent to first position described in pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505f64e-b18a-41ec-87df-ac2417829f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = sim.get_agent(0)\n",
    "agent_state = habitat_sim.AgentState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00faf613-129b-4804-a1ba-c62fca590431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique positions: {len(unique_position_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1938b-fdc7-439b-9691-422665189617",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_position_idx = 0\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# -------- Turn this into a function -------------\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "agent_pos = unique_position_list[unique_position_idx]\n",
    "agent_rotation = unique_rotation_list[unique_position_idx]\n",
    "\n",
    "print(f'Snapping agent to: {agent_pos}')\n",
    "\n",
    "agent_rotation_q = mn.Quaternion.from_matrix(mn.Matrix3(agent_rotation))\n",
    "agent_rotation = utils.quat_from_magnum(agent_rotation_q)\n",
    "\n",
    "agent_state.position = agent_pos - np.array([0, sim_settings['sensor_height'], 0])\n",
    "agent_state.rotation = agent_rotation\n",
    "agent.set_state(agent_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8200a-b305-4785-9237-7c96c0d45759",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = sim.get_sensor_observations()\n",
    "rgb = observations['color_sensor']\n",
    "depth = observations['depth_sensor']\n",
    "display_sample(rgb, np.array([]), depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfeb2a7-8c30-4fbb-9674-d7c4fc406ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sim.pathfinder.is_navigable(agent_pos))\n",
    "print(agent.get_state().position)\n",
    "print(agent.get_state().rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fce348-cee2-4d64-bb0e-2b2be1634065",
   "metadata": {},
   "source": [
    "## Scene Position Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c9f0a-674a-4a8c-89b6-d001b5f7a2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6f726-fb13-47d5-9960-8564cc856f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_per_pixel = 0.025  # @param {type:\"slider\", min:0.01, max:1.0, step:0.01}\n",
    "# @markdown Customize the map slice height (global y coordinate):\n",
    "custom_height = False  # @param {type:\"boolean\"}\n",
    "height = 1  # @param {type:\"slider\", min:-10, max:10, step:0.1}\n",
    "\n",
    "print(f\"Showing map for scan: {scene_id}\")\n",
    "print(\"The NavMesh bounds are: \" + str(sim.pathfinder.get_bounds()))\n",
    "if not custom_height:\n",
    "    # get bounding box minimum elevation for automatic height\n",
    "    height = sim.pathfinder.get_bounds()[0][1]\n",
    "\n",
    "if not sim.pathfinder.is_loaded:\n",
    "    print(\"Pathfinder not initialized, aborting.\")\n",
    "else:\n",
    "    if display:\n",
    "        agent_pos = agent_state.position\n",
    "        \n",
    "        hablab_topdown_map = maps.get_topdown_map(\n",
    "            sim.pathfinder, height=agent_pos[1], meters_per_pixel=meters_per_pixel\n",
    "        )\n",
    "        recolor_map = np.array(\n",
    "            [[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8\n",
    "        )\n",
    "        \n",
    "        hablab_topdown_map = recolor_map[hablab_topdown_map]\n",
    "        grid_dimensions = (hablab_topdown_map.shape[0], hablab_topdown_map.shape[1])\n",
    "        print(f'grid_dimensions: {grid_dimensions}')\n",
    "        \n",
    "        agent_grid_pos = maps.to_grid(agent_pos[2], agent_pos[0], grid_dimensions, pathfinder=sim.pathfinder)\n",
    "\n",
    "        agent_forward = utils.quat_to_magnum(\n",
    "            sim.agents[0].get_state().rotation\n",
    "        ).transform_vector(mn.Vector3(0, 0, -1.0))\n",
    "        agent_orientation = math.atan2(agent_forward[0], agent_forward[2])\n",
    "        \n",
    "        maps.draw_agent(hablab_topdown_map, agent_grid_pos, agent_orientation, agent_radius_px=8)\n",
    "        \n",
    "        #print(\"Displaying the map from the Habitat-Lab maps module:\")\n",
    "        display_map(hablab_topdown_map)\n",
    "        \n",
    "        # easily save a map to file:\n",
    "        #map_filename = f\"top_down_map.png\"\n",
    "        #imageio.imsave(f'/home/wes/Documents/test_development/outputs/{map_filename}', hablab_topdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98aad2-e6e6-433e-af9e-429654fedb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state.position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439f1e9-045b-40c2-bf69-edbe0c16feda",
   "metadata": {},
   "source": [
    "## Test: Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359cdd1-db97-4f0f-8f83-6be25eaa48ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time = 10\n",
    "continuous_nav = True\n",
    "\n",
    "control_frequency = 5\n",
    "frame_skip = 12\n",
    "\n",
    "fps = control_frequency * frame_skip # 60 fps\n",
    "print(f\"fps: {fps}\")\n",
    "\n",
    "control_sequence = []\n",
    "for action in range(int(sim_time * control_frequency)):  #120 actions, spread across 60 fps\n",
    "    control_sequence.append(\n",
    "        {\n",
    "            \"rotation_velocity\" : -0.3 * 2.0,\n",
    "            \"forward_velocity\" : 0.1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "observations = []\n",
    "\n",
    "velocity_control = habitat_sim.physics.VelocityControl()\n",
    "velocity_control.controlling_lin_vel = True\n",
    "velocity_control.lin_vel_is_local = True\n",
    "velocity_control.controlling_ang_vel = True\n",
    "velocity_control.ang_vel_is_local = True\n",
    "\n",
    "# manually controlling the object's kinematic state via velocity integration\n",
    "time_step = 1.0 / (frame_skip * control_frequency) # 1 / fps\n",
    "\n",
    "for i, action in enumerate(control_sequence):\n",
    "    velocity_control.linear_velocity = np.array([0, 0, -action[\"forward_velocity\"]])\n",
    "    velocity_control.angular_velocity = np.array([0, action[\"rotation_velocity\"], 0])\n",
    "\n",
    "    for frame in range(frame_skip):\n",
    "        #agent_state = agent.state\n",
    "        previous_rigid_state = habitat_sim.RigidState(utils.quat_to_magnum(agent_state.rotation), agent_state.position)\n",
    "    \n",
    "        # manually integrating the rigid state\n",
    "        target_rigid_state = velocity_control.integrate_transform(time_step, previous_rigid_state)\n",
    "    \n",
    "        # snap rigid state to navmesh and set state to object/agent\n",
    "        end_pos = sim.step_filter(previous_rigid_state.translation, target_rigid_state.translation)\n",
    "\n",
    "        if False: #frame < 10 and i < 2:\n",
    "            print(f'agent_state.position {frame}: {agent_state.position}')\n",
    "            print(f'previous_rigid_state {frame}: {previous_rigid_state.translation}')\n",
    "            print(f'target_rigid_state {frame}: {target_rigid_state.translation}')\n",
    "            print(f'end_pos {frame}: {end_pos}')\n",
    "    \n",
    "        # setting the computer state\n",
    "        agent_state.position = end_pos # + np.array([0, height, 0])\n",
    "        agent_state.rotation = utils.quat_from_magnum(target_rigid_state.rotation)\n",
    "        agent.set_state(agent_state)\n",
    "    \n",
    "        # checking if a collision occured\n",
    "        dist_moved_before_filter = (target_rigid_state.translation - previous_rigid_state.translation).dot()\n",
    "        dist_moved_after_filter = (end_pos - previous_rigid_state.translation).dot()\n",
    "    \n",
    "        EPS = 1e-5\n",
    "        collided = (dist_moved_after_filter + EPS) < dist_moved_before_filter\n",
    "        if collided:\n",
    "            print(f'collision on frame: {frame}, action: {i}')\n",
    "    \n",
    "        sim.step_physics(time_step)\n",
    "        observations.append(sim.get_sensor_observations())\n",
    "\n",
    "print(f\"frames: {str(len(observations))}\")\n",
    "vut.make_video(\n",
    "    observations = observations,\n",
    "    primary_obs = \"color_sensor\",\n",
    "    primary_obs_type = \"color\",\n",
    "    video_file = os.path.join(output_dir_path, \"test_video\"),\n",
    "    fps = fps,\n",
    "    open_vid = show_video,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f118e-b0e7-4c78-b92e-0fe6c42aa0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state.position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44958c6e-c1d8-4869-b368-4be466690a83",
   "metadata": {},
   "source": [
    "## Creating panoramic view of current position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c6d1b-4a18-4a11-9063-6d3b5cff65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapping the agent back to original orientation\n",
    "agent_state.position = agent_pos\n",
    "agent_state.rotation = agent_rotation\n",
    "agent.set_state(agent_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb652265-a9be-4206-9d22-54c769481be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_state.position)\n",
    "print(agent_state.rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14b26f-9b75-4172-9a44-2ee9765e272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_captures = int(360 / sim_settings[\"hfov\"])\n",
    "rotation_size = sim_settings[\"hfov\"]\n",
    "mid_index = int(number_of_captures/2)\n",
    "color_observations = []\n",
    "\n",
    "for i in range(number_of_captures):\n",
    "    agent_state = agent.get_state()\n",
    "    agent_state.rotation = habitat_sim.utils.quat_from_angle_axis(\n",
    "            np.deg2rad(-rotation_size * i), np.array([0, 1, 0])  # Y-axis rotation\n",
    "        )\n",
    "    agent.set_state(agent_state)\n",
    "    observations = sim.get_sensor_observations()\n",
    "    color_observations.append(observations['color_sensor'])\n",
    "\n",
    "color_observations = color_observations[-mid_index:] + color_observations[:mid_index]\n",
    "\n",
    "agent_views = {\n",
    "    \"forward\" : color_observations[2],\n",
    "    \"left\" : color_observations[1],\n",
    "    \"right\" : color_observations[3],\n",
    "    \"behind\": color_observations[0]\n",
    "}\n",
    "\n",
    "\n",
    "panoramic_image = cv2.hconcat(color_observations)\n",
    "\n",
    "'''\n",
    "color_observations = [obs[:,:,:3] for obs in color_observations]\n",
    "\n",
    "stitcher = cv2.Stitcher_create()\n",
    "status, pano = stitcher.stitch(color_observations)\n",
    "\n",
    "\n",
    "cv2.imshow('Ego-centric panoramic image', pano)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "'''\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(panoramic_image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461461dd-1ff9-4290-b9be-939416739937",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7b810-4931-474d-9a5b-550d2a0dc035",
   "metadata": {},
   "source": [
    "# Testing out VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc62616-d63f-4448-b6ec-d7036d2d9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203c05b-977a-43cf-b5a9-669bb7b34fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9d5f6-1e8e-450d-8fcc-62d06e6d07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image = Image.fromarray(agent_views[\"left\"])\n",
    "forward_image = Image.fromarray(agent_views[\"forward\"])\n",
    "right_image = Image.fromarray(agent_views[\"right\"])\n",
    "behind_image = Image.fromarray(agent_views[\"behind\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc71eec-0717-4b46-8593-cb5d9d563f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "views = [\"forward\", \"left\", \"right\", \"behind\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07bd210-4a1e-4b26-8aad-7a911a73dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prompts = {\n",
    "    \"forward\" : \"This is your view directly in front of you\",\n",
    "    \"left\" : \"This is your view 90 degrees to the left of you\",\n",
    "    \"right\" : \"This is your view 90 degrees to the right of you\",\n",
    "    \"behind\" : \"This is your view directly behind you\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e407229-9d86-4281-aff6-74f7413e82d8",
   "metadata": {},
   "source": [
    "## Agent view## Agent view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb000594-e8de-480c-a8e4-222f7169a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_view_descriptions(view_direction, messages, image_prompts, processor):\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text = [text],\n",
    "        images = image_inputs,\n",
    "        videos = video_inputs,\n",
    "        padding = True,\n",
    "        return_tensors = \"pt\",\n",
    "    )\n",
    "\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d7bbd-9567-477e-836c-90549a706aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_description = {}\n",
    "for view in views:\n",
    "    messages = [\n",
    "        {\"role\" : \"system\",\n",
    "         \"content\" : \"You are an embodied agent tasked with understanding your environment. You are provided images from your sensors and will describe what you see in less than 50 words.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": Image.fromarray(agent_views[view])},\n",
    "                {\"type\": \"text\", \"text\": image_prompts[view]},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    view_description[view] = agent_view_descriptions(view, messages, image_prompts, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9ced9-48a6-4568-96a5-47d770758a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(view_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cdda4-f7ac-4dcb-8575-c630f03ae239",
   "metadata": {},
   "source": [
    "## Setting the broad agent instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c199b9-a70c-4313-a5ac-d048a2e8c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\"role\" : \"system\",\n",
    "         \"content\" : \n",
    "             '''You are an embodied agent tasked with following a set of instructions provided to you in order to navigate around an environment.\n",
    "             Take the navigation instructions provided to you and create a list of steps that you must follow in order to achieve your goal.'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": f\"Navigation instructions: {nav_instruction}\"},\n",
    "            ],\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e30f76-1006-48c7-8467-7d52e2b91a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "        text = [text],\n",
    "        images = image_inputs,\n",
    "        videos = video_inputs,\n",
    "        padding = True,\n",
    "        return_tensors = \"pt\",\n",
    "    )\n",
    "\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "nav_steps = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf8ad9-7e50-43fd-91d2-25a7edc5a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Navigation Instructions: {nav_instruction}')\n",
    "print()\n",
    "print(f'List of discrete actinos: {nav_steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d2288-5c13-4d39-91f4-15eb0357d6cf",
   "metadata": {},
   "source": [
    "## Putting it together with the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0a718-f148-4469-89b0-8fdc331302d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\"role\" : \"system\",\n",
    "         \"content\" : \n",
    "             f'''You are an embodied agent tasked with following a set of instructions provided to you in order to navigate around an environment.\n",
    "             Your navigation instructions are: {nav_steps}.\n",
    "             Use the images from your sensors provided to you to select an action from the following list: Move Forward, Turn Left, Turn Right, Turn to face behind you, Goal Reached'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \n",
    "                f'''\n",
    "                To your front you see: {view_description[\"forward\"]},\n",
    "                To your left you see: {view_description[\"left\"]},\n",
    "                To your right you see: {view_description[\"right\"]},\n",
    "                Directly behind you see: {view_description[\"behind\"]}\n",
    "                '''},\n",
    "            ],\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d19dc-825f-4944-8af2-96db6bacdd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "        text = [text],\n",
    "        images = image_inputs,\n",
    "        videos = video_inputs,\n",
    "        padding = True,\n",
    "        return_tensors = \"pt\",\n",
    "    )\n",
    "\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "next_step = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5d01f-b93f-4d91-b821-7d73b10344f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e41831-1124-4fab-948f-c57dba7f9b7b",
   "metadata": {},
   "source": [
    "# Taking an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa972e-54ac-4072-b4fc-ac399f0de725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
