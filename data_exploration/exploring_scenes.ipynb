{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6902ac7-d79c-4564-80f7-17ba4aedf2bb",
   "metadata": {},
   "source": [
    "# To-Do next:\n",
    "1. ~~Make `scene_path` update using `get_scene_path`~~\n",
    "2. ~~Update `scene` in `sim_settings` to use the new `scene_path`~~\n",
    "3. ~~Fix the positioning of agent on the map (done?)~~\n",
    "4. ~~Create a video showing the agent spinning around to demonstrate environment~~\n",
    "5. ~~Check to see if there is a better way to account for agent height (?)~~\n",
    "6. ~~Work out 360 view~~ _(Assuming that this is not required for now)_\n",
    "7. ~~Work out prompt for agent to describe task~~\n",
    "8. ~~Hook up a ViT~~\n",
    "9. ~~Make agent take a series of steps~~\n",
    "10. ~~Plot trajectory on a map~~\n",
    "11. ~~Try using GPT4 LLM as decision-maker~~\n",
    "12. ~~Try injecting a panoramic view into the decision maker to help ground the decision making~~ (limited / no noticeable benefit)\n",
    "13. ~~Try injecting a panoramic view into the view description~~ (limited / no noticeable benefit)\n",
    "14. ~~Try injecting the navigation instructions into the view descriptions~~ (some benefit, but is leading to stronger confidence in hallucinations)\n",
    "15. ~~Make agent able to iterate through multiple trajectories~~\n",
    "16. ~~Make agent able to iterate through multiple scenes~~\n",
    "17. Try using GPT4 LLM with full conversation history as decision-maker\n",
    "18. ~~Try using GPT4 to self-prompt~~\n",
    "19. Try creating an orthogonal orientator?\n",
    "20. ~~Provide agent with environment collision data~~\n",
    "21. Test checking the chosen direction view after selecting an initial action\n",
    "22. ~~Check error distance per action taken~~\n",
    "23. ~~Return an output dictionary of reasoning and action taken per step (for posterity?)~~\n",
    "24. ~~Run different scans~~\n",
    "25. ~~Run multiple scans~~\n",
    "26. ~~Plot error distances~~\n",
    "27. ~~Improve action selection robustness (enforce LLM selection more tightly, try/except, regex)~~\n",
    "28. ~~Add ability to save outputs~~\n",
    "29. ~~Add ability to load outputs~~\n",
    "30. ~~Add ability to work out proportional progress~~\n",
    "31. Try comparing observations to panorama to generate correct understandings\n",
    "32. Try using a more sophisticated model (e.g. `o1`)\n",
    "33. Implement a stronger `past_action_story` to better help navigate progress\n",
    "34. Implement a better `Navigation Checkpoints` to help ground the model in understanding where it is\n",
    "35. Read up on current implementations on zero/few-shot VLN to get some ideas\n",
    "36. ~~Reject training trajectories that cannot be achieved within 50 steps~~\n",
    "37. ~~Implement more granular rotation / views (45 degrees?)~~\n",
    "38. Implement training LLM prompts\n",
    "39. ~~Structure training loop / functions~~\n",
    "40. ~~Structure training data~~\n",
    "41. ~~Finetune~~\n",
    "42. Re-finetune model with data that excludes indents\n",
    "43. Create calibration dataset for finetuning\n",
    "44. Quantize finetuned model\n",
    "45. Run a series (100?) of testing trajectories and determine initial effectiveness of prompts\n",
    "46. Tweak prompts until satisfied\n",
    "47. Re-generate training data\n",
    "48. Re-run testing trajectories to get results\n",
    "\n",
    "# Major milestones\n",
    "1. ~~Map agent in environment~~\n",
    "2. ~~Agent random-walk with position trace~~\n",
    "3. Work out 360 view\n",
    "4. ~~Hook up vision transformer~~\n",
    "5. ~~Hook up LLM~~\n",
    "7. Hook up second vision transformer\n",
    "8. ??\n",
    "9. Write-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1552b12-0fa0-40ad-b895-4cb9eb8a75ea",
   "metadata": {},
   "source": [
    "# Basic Schematic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6009b5c-e05e-44a7-b5ce-3061bdbdfc13",
   "metadata": {},
   "source": [
    "![Basic Schematic of Modelling Approach](basic_schematic.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5639c605",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    " - \"Hop over the coffee table\" (scan_id: S9hNv5qa7GM, instruction_id: 167)\n",
    " - Uses outdoor spiral staircase (scan_id: PX4nDJXEHrG, instruction_id: 2460)\n",
    " - \"Hop over the table with the white chairs\" (scan_id: SN83YJsR3w2, instruction_id: 651)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5630b",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ad8b7-7581-4ba3-abc1-766bf4857e0f",
   "metadata": {},
   "source": [
    "# 2. Setup\n",
    "This section is used to set up the workbook by:\n",
    "1. Importing required modules\n",
    "2. Establishing Folder Paths and Directories\n",
    "3. Defining required functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1483f58",
   "metadata": {},
   "source": [
    "## 2.1 Setup: Required module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0df8e-8d08-49b4-9ec4-3d09822df556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import magnum as mn\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import textwrap\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gc\n",
    "import magnum as mn\n",
    "import quaternion\n",
    "from quaternion import from_rotation_vector\n",
    "from itertools import cycle\n",
    "\n",
    "# Habitat-sim related imports\n",
    "import habitat_sim\n",
    "from habitat_sim.utils import common as utils\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "from habitat.utils.visualizations import maps\n",
    "\n",
    "# VLM and LLM related imports\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForVision2Seq\n",
    "from awq.models.qwen2vl import Qwen2VLAWQForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from openai import OpenAI\n",
    "from accelerate import Accelerator\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d9d76-1e4e-4256-9457-fa91b84134ed",
   "metadata": {},
   "source": [
    "## 2.2 Setup: Directory paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508ac6c-f689-4888-942e-6a230bbaa51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core home/root folder paths\n",
    "habitat_dir_path = \"/home/wes/habitat/data/\"                                        # path to habitat-sim root directory\n",
    "rxr_data_path = \"/media/wes/Lexar/rxr_data/rxr-data/\"                               # path to room-across-room data directory\n",
    "training_dir_path = \"/home/wes/Documents/training_trajectories/\"                    # trajectories, reasoning, and actions used as finetuning data\n",
    "output_dir_path = \"/home/wes/Documents/outputs/\"                                    # path to model outputs\n",
    "\n",
    "# Folder paths based on root folder paths\n",
    "scan_dir_path = os.path.join(habitat_dir_path, f\"scene_datasets/mp3d/\")             # directory of habitat-sim scenes\n",
    "pose_dir_path = os.path.join(rxr_data_path, f'pose_traces/rxr_train/')              # directory of the RxR training poses\n",
    "test_pose_dir_path = os.path.join(rxr_data_path, f'pose_traces/rxr_val_unseen/')    # directory of the RxR testing poses\n",
    "mp3d_scene_config = os.path.join(scan_dir_path, f\"mp3d.scene_dataset_config.json\")  # filepath of Matterport 3D scene configuration file\n",
    "\n",
    "scan_path = '' # declaring global variable for updating scene path as we iterate from scene-to-scene\n",
    "\n",
    "training_annotations_fname = \"rxr_train_guide.jsonl/rxr_train_guide.json\"           # filepath to the training annotations provided by RxR\n",
    "training_annotations_path = os.path.join(rxr_data_path, training_annotations_fname) # full filepath to training annotations provided by RxR\n",
    "\n",
    "testing_annotations_fname = \"rxr_val_unseen_guide.jsonl/rxr_val_unseen_guide.json\"  # filepath to the validation (unseen) annotations provided by RxR\n",
    "testing_annotations_path = os.path.join(rxr_data_path, testing_annotations_fname)   # full filepath to validation (unseen) annotations provided by RxR\n",
    "\n",
    "# checking that paths exist\n",
    "print(f'Exists: {os.path.exists(scan_dir_path)} \\t scan_dir_path: {scan_dir_path}')\n",
    "print(f'Exists: {os.path.exists(pose_dir_path)} \\t pose_dir_path: {pose_dir_path}')\n",
    "print(f'Exists: {os.path.exists(pose_dir_path)} \\t pose_dir_path: {test_pose_dir_path}')\n",
    "print(f\"Exists: {os.path.exists(mp3d_scene_config)} \\t mp3d_scene_config: {mp3d_scene_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1412c8-7272-4ddb-9fb4-300a87c22683",
   "metadata": {},
   "source": [
    "## 2.3. Setup: Declaration of global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ec6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global simulator variables for use across multiple functions\n",
    "cfg = None\n",
    "sim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e318aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default angles for each 'sensor' available for use to observe the environment\n",
    "# Each observation is created by rotating the single color camera on the agent agent by a set angle\n",
    "view_rotations = {\n",
    "    \"forward\": 0.0,\n",
    "    \"forward-right\": -math.pi/4,\n",
    "    \"right\": -math.pi/2,\n",
    "    \"behind\": math.pi,\n",
    "    \"forward-left\": math.pi/4,\n",
    "    \"left\": math.pi/2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ba2e0",
   "metadata": {},
   "source": [
    "## 2.4 Setup: Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23061fc2",
   "metadata": {},
   "source": [
    "### 2.4.1 Setup: Update Functions - Updates to file paths, saving and loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6118f18c-3cee-4f37-9be5-ba4b1dc21c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the full filepath of a scene given a scene id (i.e. scan id)\n",
    "def get_scan_path(scene_id):\n",
    "    global scan_dir_path\n",
    "    return os.path.join(scan_dir_path, f\"{scene_id}/{scene_id}.glb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd701717-c2f8-4ffd-afa8-016489c1229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate the new scene filepath across all relevant dictionaries, etc.\n",
    "def update_scan_path(scene_id):\n",
    "    global scan_dir_path\n",
    "    global sim_settings\n",
    "    \n",
    "    scan_path = get_scan_path(scene_id)\n",
    "    sim_settings['scene'] = scan_path\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c11c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the outputs / results in a pickle file\n",
    "def save_results(data, dir_path, fname):\n",
    "    fpath = dir_path + fname + '.pkl'\n",
    "    print(f'Saving to: {fpath}')\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data.to_pickle(fpath)\n",
    "    else:\n",
    "        with open(fpath, 'wb') as file:\n",
    "            pickle.dump(data, file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d93c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a file\n",
    "def load_results(filepath, open_mode='rb'):\n",
    "    output = None\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, open_mode) as file:\n",
    "            output = pickle.load(file)\n",
    "            print(f\"File loaded: {filepath}\")\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ce0c9-b159-40c3-841c-ac884d42c080",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4.2 Setup: Utility Functions - Displaying Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6ec995-cb0f-4f73-be60-288d87c61fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying a single sample of an observation\n",
    "def display_sample(rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([])):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [\"rgb\", \"depth\"]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 2, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show(block=False)\n",
    "\n",
    "# Function to create a video when in Continuous Environment\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--no-display\", dest=\"display\", action=\"store_false\")\n",
    "    parser.add_argument(\"--no-make-video\", dest=\"make_video\", action=\"store_false\")\n",
    "    parser.set_defaults(show_video=True, make_video=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    show_video = args.display\n",
    "    display = args.display\n",
    "    do_make_video = args.make_video\n",
    "else:\n",
    "    show_video = False\n",
    "    do_make_video = False\n",
    "    display = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2765a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves an agent's current observations from its color sensors\n",
    "def get_current_observations(agent, directions=None):\n",
    "    agent_views = dict()\n",
    "    original_state = agent.get_state()\n",
    "    new_state = habitat_sim.AgentState()\n",
    "    color_observations = []\n",
    "\n",
    "    # Default observations are in 90 degree rotations\n",
    "    if directions is None:\n",
    "        directions = {\n",
    "            \"forward\": 0.0,\n",
    "            \"left\": math.pi/2,\n",
    "            \"right\": -math.pi/2,\n",
    "            \"behind\": math.pi\n",
    "        }\n",
    "    \n",
    "    for direction, angle in directions.items():\n",
    "        rotation_quat = from_rotation_vector([0, angle, 0])\n",
    "        new_orientation = original_state.rotation * rotation_quat\n",
    "        \n",
    "        # Rotating the agent\n",
    "        new_state.position = original_state.position\n",
    "        new_state.rotation = new_orientation\n",
    "        agent.set_state(new_state)\n",
    "        \n",
    "        # Getting the observations\n",
    "        observations = sim.get_sensor_observations()\n",
    "        color_observations.append(observations[\"color_sensor\"])\n",
    "        agent_views[direction] = observations[\"color_sensor\"]\n",
    "        \n",
    "    # Returning agent to original orientation\n",
    "    agent.set_state(original_state)\n",
    "\n",
    "    return agent_views, color_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db5b275f-2827-4387-befa-34d69d8bc762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create basic forward-centred panoramic image\n",
    "def get_current_panorama(agent, number_of_captures = 5):\n",
    "    rotation_size = 360/number_of_captures\n",
    "    initial_orientation = agent.get_state().rotation\n",
    "    \n",
    "    pano_observations = []\n",
    "\n",
    "    for i in range(number_of_captures + 1):  # +1 to return agent back to original orientation\n",
    "        agent_state = agent.get_state()\n",
    "        rotation_amount = habitat_sim.utils.quat_from_angle_axis(\n",
    "            np.deg2rad(-rotation_size * i), np.array([0,1,0]) # Clockwise rotation about the Y-axis\n",
    "        )\n",
    "        agent_state.rotation = initial_orientation * rotation_amount\n",
    "        \n",
    "        agent.set_state(agent_state)\n",
    "        observations = sim.get_sensor_observations()\n",
    "        pano_observations.append(observations[\"color_pano_sensor\"])\n",
    "\n",
    "    pano_observations = pano_observations[:-1]\n",
    "    \n",
    "    return cv2.hconcat(pano_observations[-2:] + pano_observations[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b10467-3597-4957-af01-020422dd93ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4.3 Setup: Utility Functions - Configure Habitat-Sim Simulator Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2a1f834-b92d-4cb8-8861-3fe93ea19209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up default sim settings\n",
    "rgb_sensor = True\n",
    "depth_sensor = True\n",
    "semantic_sensor = False\n",
    "seed = 1\n",
    "number_images_for_panorama = 4 # number of images required for a full 360 degree panorama to be created\n",
    "\n",
    "sim_settings = {\n",
    "    \"width\": 64,  # Spatial resolution of the observations\n",
    "    \"height\": 64,\n",
    "    \"scene\": scan_path,  # Scene path\n",
    "    \"scene_dataset\": mp3d_scene_config,  # the scene dataset configuration files\n",
    "    \"default_agent\": 0,\n",
    "    \"sensor_height\": 1.4,  # Height of sensors in meters\n",
    "    \"color_sensor\": rgb_sensor,  # RGB sensor\n",
    "    \"depth_sensor\": depth_sensor,  # Depth sensor\n",
    "    \"semantic_sensor\": semantic_sensor,  # Semantic sensor\n",
    "    \"seed\": seed,  # used in the random navigation\n",
    "    \"enable_physics\": False,  # kinematics only    \n",
    "    \"hfov\" : 90, #360 / number_images_for_panorama\n",
    "    \"step_size\" : 1.0 # metres moving forward\n",
    "}\n",
    "\n",
    "sensor_height_adjustment = np.array([0, -sim_settings[\"sensor_height\"], 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "053ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success criteria and maximum number of steps allowed in each trajectory (to limit experience time)\n",
    "min_waypoint_distance = sim_settings[\"step_size\"] / 2\n",
    "min_goal_distance = 1.0\n",
    "max_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea2168b9-4791-46a7-938a-01db7187f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a simulator based on the simulator settings specified in sim_settings\n",
    "def make_cfg(settings):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.scene_id = settings[\"scene\"]\n",
    "    sim_cfg.scene_dataset_config_file = settings[\"scene_dataset\"]\n",
    "    sim_cfg.enable_physics = settings[\"enable_physics\"]\n",
    "\n",
    "    sensor_specs = []\n",
    "\n",
    "    # RGB sensor specs: used for colour observations for the agent\n",
    "    color_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    color_sensor_spec.uuid = \"color_sensor\"\n",
    "    color_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    color_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    color_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    color_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    color_sensor_spec.hfov = settings[\"hfov\"]\n",
    "    sensor_specs.append(color_sensor_spec)\n",
    "\n",
    "    # RGB panorama sensor specs: used to create a 360 degree panorama, with \"forward\" as centre image\n",
    "    color_sensor_pano_spec = habitat_sim.CameraSensorSpec()\n",
    "    color_sensor_pano_spec.uuid = \"color_pano_sensor\"\n",
    "    color_sensor_pano_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    color_sensor_pano_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    color_sensor_pano_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    color_sensor_pano_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    color_sensor_pano_spec.hfov = 360/5\n",
    "    sensor_specs.append(color_sensor_pano_spec)\n",
    "\n",
    "    # Depth sensor specs\n",
    "    depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    depth_sensor_spec.uuid = \"depth_sensor\"\n",
    "    depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "    depth_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    depth_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    depth_sensor_spec.hfov = settings[\"hfov\"]\n",
    "    sensor_specs.append(depth_sensor_spec)\n",
    "\n",
    "    # Agent movement: Predefined agent movements\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "    agent_cfg.action_space = {\n",
    "        \"move_forward\" : habitat_sim.agent.ActionSpec(\n",
    "            \"move_forward\", habitat_sim.agent.ActuationSpec(amount=settings[\"step_size\"])\n",
    "        ),\n",
    "        \"turn_left\":habitat_sim.agent.ActionSpec(\n",
    "            \"turn_left\", habitat_sim.agent.ActuationSpec(amount=90.0)\n",
    "        ),\n",
    "        \"turn_right\":habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=90.0)\n",
    "        ),\n",
    "        \"turn_around\":habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=180.0)\n",
    "        ),\n",
    "        \"turn_forwardleft\":habitat_sim.agent.ActionSpec(\n",
    "            \"turn_left\", habitat_sim.agent.ActuationSpec(amount=45.0)\n",
    "        ),\n",
    "        \"turn_forwardright\":habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=45.0)\n",
    "        )\n",
    "\n",
    "    }\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd40cb6-7492-4f23-aa5b-165a97a6764b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4.4 Setup: Utility Functions - Map agent position to a topdown map of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00873423-eeab-45b9-a290-8af06fd59c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 3d points to 2d topdown coordinates\n",
    "def convert_points_to_topdown(pathfinder, points, meters_per_pixel = 0.025):\n",
    "    points_topdown = []\n",
    "    bounds = pathfinder.get_bounds()\n",
    "    for point in points:\n",
    "        # convert 3D x,z to topdown x,y\n",
    "        px = (point[0] - bounds[0][0]) / meters_per_pixel\n",
    "        py = (point[2] - bounds[0][2]) / meters_per_pixel\n",
    "        points_topdown.append(np.array([px, py]))\n",
    "    return points_topdown\n",
    "\n",
    "\n",
    "# Display a topdown map with matplotlib\n",
    "def display_map(topdown_map, key_points=None, gold_path=None):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow(topdown_map)\n",
    "    \n",
    "    # plot points visited\n",
    "    if key_points is not None:\n",
    "        xs = np.array([point[0] for point in key_points])\n",
    "        ys = np.array([point[1] for point in key_points])\n",
    "        plt.plot(xs, ys, marker=\"o\", markersize=6, alpha=0.6, color='lightsteelblue')\n",
    "\n",
    "    # plot points visited in the provided by 'gold label' RxR data\n",
    "    if gold_path is not None:\n",
    "        xs = np.array([point[0] for point in gold_path])\n",
    "        ys = np.array([point[1] for point in gold_path])\n",
    "        plt.plot(xs, ys, marker=\"x\", markersize=4, linestyle='dashed', linewidth=1, alpha=0.5, color='gold')\n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80c3f1bc-833a-4177-a40c-e07a096d5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the map of the current scene\n",
    "def draw_map(agent, scene_id, meters_per_pixel = 0.025, height = 1, agent_radius_px = 8):\n",
    "    #print(f\"Showing map for scene: {scene_id}\")\n",
    "    #print(f\"The NavMesh bounds are: {str(sim.pathfinder.get_bounds())}\")\n",
    "    \n",
    "    agent_pos = agent.get_state().position\n",
    "    agent_forward = utils.quat_to_magnum(agent.get_state().rotation).transform_vector(mn.Vector3(0,0,-1.0))\n",
    "    agent_orientation = math.atan2(agent_forward[0], agent_forward[2])\n",
    "\n",
    "    topdown_map = maps.get_topdown_map(\n",
    "        sim.pathfinder, height=agent_pos[1], meters_per_pixel = meters_per_pixel\n",
    "    )\n",
    "    recolor_map = np.array(\n",
    "        [[255,255,255],[128,128,128], [0,0,0]], dtype=np.uint8\n",
    "    )\n",
    "    topdown_map = recolor_map[topdown_map]\n",
    "\n",
    "    grid_dimensions = (topdown_map.shape[0], topdown_map.shape[1])\n",
    "    agent_grid_pos = maps.to_grid(agent_pos[2], agent_pos[0], grid_dimensions, pathfinder=sim.pathfinder)\n",
    "    maps.draw_agent(topdown_map, agent_grid_pos, agent_orientation, agent_radius_px=agent_radius_px)\n",
    "\n",
    "    return topdown_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a417ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the map and trajectory taken for a particular trajectory\n",
    "def show_trajectory(scan_id, instruction_id, trajectory_agent_states, gold_path_positions):\n",
    "    global cfg\n",
    "    global sim\n",
    "    \n",
    "    try:\n",
    "        sim.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    update_scan_path(scan_id)\n",
    "    cfg = make_cfg(sim_settings)\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "    agent = sim.get_agent(0)\n",
    "\n",
    "    trajectory_positions, trajetory_rotations = zip(\n",
    "        *[(state.position, state.rotation) for state in trajectory_agent_states]\n",
    "    )\n",
    "\n",
    "    agent_state = habitat_sim.AgentState()\n",
    "    agent_state = snap_to_point(agent_state, trajectory_positions[-1], trajetory_rotations[-1])\n",
    "    agent.set_state = agent_state\n",
    "\n",
    "    topdown_map = draw_map(agent, scan_id)\n",
    "    trajectory = convert_points_to_topdown(sim.pathfinder, trajectory_positions)\n",
    "    gold_path = convert_points_to_topdown(sim.pathfinder, gold_path_positions)\n",
    "\n",
    "    display_map(topdown_map, key_points=trajectory, gold_path=gold_path)\n",
    "\n",
    "    sim.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925119b-6489-4210-9be1-d47d5b81cc54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4.5 Setup: Utility Funtions - Reading and loading RxR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25a0e16d-08ae-4069-b259-b7a50cb9003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read RxR data and return it as a dataframe\n",
    "def json_to_df(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return pd.DataFrame(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6149937a-2edb-43a5-835b-10d5d4537bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a scan id (aka: scene_id) and annotation file path\n",
    "# Returns a list of scene_ids and associated instruction_ids and instructions\n",
    "# If scan_id is provided, then only annotations and instruction ids for the scene_id is returned\n",
    "def get_rxr_annotations(annotations_path, scan_id_list = 'None'):\n",
    "    if not os.path.exists(annotations_path):\n",
    "        print(f'Annotation path not found: {annotations_path}')\n",
    "        return\n",
    "\n",
    "    n_records = 0\n",
    "    rxr_annotations = {}\n",
    "    # Turning entire annotation json into dataframe\n",
    "    df_rxr = json_to_df(annotations_path)\n",
    "    \n",
    "    # Focusing on just the english instructions for now\n",
    "    df_rxr = df_rxr[df_rxr['language']=='en-US']\n",
    "\n",
    "    # If no scan_id provided, go through entire dataset\n",
    "    if scan_id_list == 'None':\n",
    "        scan_id_list = df_rxr['scan'].unique().tolist()\n",
    "\n",
    "    # Creates a dictionary that matches instructions and scenes\n",
    "    # key = scan_id\n",
    "    # value = [{instruction_id}:{instructions}, ...]\n",
    "    for scan_id in scan_id_list:\n",
    "        rxr_annotations[scan_id] = df_rxr[df_rxr['scan']==scan_id][['instruction_id', 'instruction']].to_dict('records')\n",
    "        n_records += len(rxr_annotations[scan_id])\n",
    "\n",
    "    return scan_id_list, rxr_annotations, n_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7004d81-551d-4bb5-9732-7b975738cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an instruction_id and returns the pose trace\n",
    "# Pose trace: the list of positions (and rotations) along the gold path\n",
    "def get_pose_trace(pose_dir_path, instruction_id):\n",
    "    instruction_id = ('000000' + str(instruction_id))[-6:] # Appending zeros to match file name nomenclature\n",
    "    pose_trace_path = os.path.join(pose_dir_path, f'{instruction_id}_guide_pose_trace.npz')\n",
    "    if not os.path.exists(pose_trace_path):\n",
    "        print(f'pose_trace_path not found: {pose_trace_path}')\n",
    "        return\n",
    "\n",
    "    pose_trace = np.load(pose_trace_path)['extrinsic_matrix']\n",
    "    return pose_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "811fbb07-4736-4a9b-9c8d-34434c6f5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a pose trace dataset and returns a list of all unique positions\n",
    "#  and a list of the rotation based on that unique position\n",
    "def get_unique_poses(pose_trace):\n",
    "    unique_position_list = np.zeros((1,3))\n",
    "    unique_rotation_list = np.zeros((1,3,3))\n",
    "    for pose in pose_trace:\n",
    "        if not np.all(np.isin(np.round(pose[:3,3], 2), unique_position_list)):\n",
    "            unique_position_list = np.append(unique_position_list, np.round(np.array(pose[:3,3]), 2).reshape(1,3), axis=0)  # position can be rounded to account for slight offsets during image capture\n",
    "            unique_rotation_list = np.append(unique_rotation_list, np.array(pose[:3,:3]).reshape(1,3,3), axis=0) # rotation should not be rounded to maintain precise viewing angles\n",
    "\n",
    "    return np.squeeze(unique_position_list[1:]), unique_rotation_list[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9853d7-f354-4c55-80b3-3a4b2391cf71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4.6 Setup: Utility Functions - Generate outputs from VLM and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a5c2b80-3c1d-4490-949a-84172bd5f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs_qwen2(model, processor, messages, max_tokens=128):\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, add_vision_id=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "            text = [text],\n",
    "            images = image_inputs,\n",
    "            videos = video_inputs,\n",
    "            padding = True,\n",
    "            return_tensors = \"pt\",\n",
    "        )\n",
    "    \n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10a280a7-cae3-4f1a-9870-c83155187f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs_openai(client, messages, model=\"gpt-4o-mini\"):\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8c409-7fcd-4e6b-8c23-6d7e41d55ce6",
   "metadata": {},
   "source": [
    "### 2.4.7 Setup: Utility Functions - Agent-Environment Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bddea21e-4cac-406f-9960-0dd25e8ddc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct the agent to take a specific action\n",
    "def take_action(agent, action):\n",
    "    if action != 'Goal':\n",
    "        action_dict = {\n",
    "            \"Forward\" : \"move_forward\",\n",
    "            \"Left\" : \"turn_left\",\n",
    "            \"Right\" : \"turn_right\",\n",
    "            \"Behind\" : \"turn_around\",\n",
    "            \"Forward-left\" : \"turn_forwardleft\",\n",
    "            \"Forward-right\" : \"turn_forwardright\"\n",
    "        }\n",
    "    \n",
    "        agent.act(action_dict[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5490c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a position and rotation state and returns an agent state\n",
    "def snap_to_point(agent_state, agent_position, agent_rotation):\n",
    "    # Turn rotation matrix into a quaternion\n",
    "    if isinstance(agent_rotation, quaternion.quaternion):\n",
    "        agent_rotation_quat = agent_rotation\n",
    "        agent_state.rotation = agent_rotation\n",
    "    else:\n",
    "        agent_rotation_quat = mn.Quaternion.from_matrix(mn.Matrix3(agent_rotation))\n",
    "        agent_state.rotation = utils.quat_from_magnum(agent_rotation_quat)  # Turn magnum quaternion into habitat-sim quaternion\n",
    "\n",
    "    agent_state.position = agent_position\n",
    "    \n",
    "    return agent_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc047990-f84a-4ba0-9c9d-a32e58c18d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if the agent can move forward without a collision\n",
    "# Acts as a proxy for taking in laser-scanned distances\n",
    "# Takes as input:\n",
    "#    1) The agent\n",
    "#    2) The direction (in radians) to check in, where 0.0 is forward\n",
    "#    3) The error margin (in metres) that the closest snap_point can be to\n",
    "#        the intended_pos and still be considered a navigable point\n",
    "def check_step_collision(agent, direction=0.0, error_radius=0.05):\n",
    "    agent_state = agent.get_state()\n",
    "    current_pos = agent_state.position\n",
    "    agent_forward = utils.quat_to_magnum(\n",
    "            agent_state.rotation\n",
    "        ).transform_vector(mn.Vector3(0, 0, -1.0))\n",
    "      \n",
    "    direction_vector = mn.Quaternion.rotation(mn.Rad(direction), mn.Vector3(0, 1, 0)).transform_vector(agent_forward)\n",
    "    \n",
    "    intended_pos = current_pos + direction_vector * sim_settings[\"step_size\"]\n",
    "    is_navigable = sim.pathfinder.is_navigable(intended_pos)\n",
    "    \n",
    "    # Checking if a snapped point would be within a small margin of the intended_pos\n",
    "    #  We will allow this snapped point to take the place of the intended_pos if\n",
    "    #  the snapped point is within a small error margin - this avoids instances\n",
    "    #  of a navigable point (with environment sliding enabled) being ruled un-navigable\n",
    "    #  if it is only off by a small margin\n",
    "    snapped_intended_pos = sim.pathfinder.snap_point(intended_pos)\n",
    "    euclidean_distance = np.linalg.norm(\n",
    "        intended_pos[[0,2]] - snapped_intended_pos[[0,2]]\n",
    "    )\n",
    "    #print(f'Euclidean Distance to snap_point: {euclidean_distance}')\n",
    "\n",
    "    if not is_navigable and euclidean_distance <= error_radius:\n",
    "        #print(f'Using snapped point instead')\n",
    "        intended_pos = snapped_intended_pos\n",
    "        is_navigable = True\n",
    "    \n",
    "    #print(f\"check_step_collsion - direction: {direction}\")\n",
    "    #print(f'Intended position is navigable? {is_navigable}')\n",
    "    \n",
    "    return is_navigable, intended_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c1e58-a7d7-4213-8293-068a69923f6e",
   "metadata": {},
   "source": [
    "### 2.4.8 Setup: Utility Functions - Reporting on Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca6b5d6f-8bf9-47a6-baa9-dfa5d9ea7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds and returns the geodesic distance between two points\n",
    "#  in the environment. Returns \"null\" if no path found\n",
    "def find_geodesic_distance(agent_positions, gold_end_pos):\n",
    "    distances = []\n",
    "    paths = []\n",
    "    path = habitat_sim.ShortestPath()\n",
    "\n",
    "    if not(isinstance(agent_positions, list)):\n",
    "        agent_positions = list(agent_positions)\n",
    "\n",
    "    for position in agent_positions:\n",
    "        # setting agent position to same height as used in dataset\n",
    "        path.requested_start = np.array(\n",
    "            [\n",
    "                position[0],\n",
    "                gold_end_pos[1],\n",
    "                position[2]\n",
    "            ]\n",
    "        )\n",
    "        path.requested_end = gold_end_pos\n",
    "        found_path = sim.pathfinder.find_path(path)\n",
    "        \n",
    "        paths.append(path.points)\n",
    "        distances.append(path.geodesic_distance)\n",
    "\n",
    "    return distances, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6ca18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the distances to the goal for all trajectories in dictionary,\n",
    "#  as well as representing that distance as proportion of the starting distances\n",
    "def calculate_performance(outputs):\n",
    "    performance = dict()\n",
    "\n",
    "    # Calculating distance as a proportion of initial starting position\n",
    "    scan_id_list = outputs.keys()\n",
    "\n",
    "    for scan_id in scan_id_list:\n",
    "        scan_performance = dict()\n",
    "        instruction_id_list = outputs[scan_id].keys()\n",
    "        \n",
    "        for instruction_id in instruction_id_list:\n",
    "            distances = outputs[scan_id][instruction_id]['distances']\n",
    "            final_distance = distances[-1]\n",
    "            final_distance_percent = None\n",
    "            if final_distance != np.inf and distances[0] != np.inf:\n",
    "                final_distance_percent = 1 - (final_distance/distances[0])\n",
    "\n",
    "            #performance.append(np.array([final_distance, final_distance_percent]))\n",
    "            scan_performance[instruction_id] = np.array([final_distance, final_distance_percent])\n",
    "\n",
    "        performance[scan_id] = scan_performance\n",
    "\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd26202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the performance dictionary and plots the Error Distance on a chart\n",
    "#  and highlights any trajectories where distance could not be calculated\n",
    "def plot_performance(outputs):\n",
    "    performance = calculate_performance(outputs)\n",
    "\n",
    "    # Creating dataframe to house data\n",
    "    df = pd.DataFrame().from_dict(performance, orient='index').stack()\n",
    "    df = df.apply(lambda x: pd.Series([x[0],x[1]]))\n",
    "    df.columns = ['Distance', 'Proportional_Distance']\n",
    "    df['Proportional_Distance'] = df['Proportional_Distance']*100\n",
    "    df['Error'] = df.apply(lambda x: 1 if pd.isna(x['Proportional_Distance']) else 0, axis=1)\n",
    "    \n",
    "    # For trajectory errors, Proportional Distance == 0, Distance == 50\n",
    "    df['Proportional_Distance'] = df['Proportional_Distance'].fillna(0)\n",
    "    df['Distance'] = df['Distance'].replace(np.inf, 50)\n",
    "    df['Label'] = df.index.map(lambda x: '_'.join(map(str,x)))\n",
    "    \n",
    "    # Plotting the performances from multiple annotations and scans\n",
    "    n_rows = np.arange(len(df['Distance']))\n",
    "    color = ['red' if x == 1 else 'blue' for x in df['Error']]\n",
    "\n",
    "    plt.barh(n_rows, df['Distance'], color=color)\n",
    "    plt.xlabel('Distance to goal')\n",
    "    plt.yticks(n_rows, labels=df['Label'])\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db380c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draws the step-by-step observations for a single trajectory\n",
    "def draw_trajectory_observations(observations, actions, reasoning):\n",
    "    global view_rotations\n",
    "\n",
    "    width = 8\n",
    "    observation_order = list(view_rotations.keys())\n",
    "    n_steps = len(observations)\n",
    "    wrap_width = 100\n",
    "\n",
    "    fig, axs = plt.subplots(n_steps, len(observation_order), figsize=(width, 2*n_steps))\n",
    "\n",
    "    # Print column headings\n",
    "    for i, direction in enumerate(observation_order):\n",
    "        fig.text(0.2 + (0.12 * i), 0.885,\n",
    "                 direction.capitalize(), ha='center', \n",
    "                 fontsize=10, fontweight='bold')\n",
    "\n",
    "    # For each action\n",
    "    for step in range(len(observations)):\n",
    "        # Print row labels\n",
    "        fig.text(-0.8, 0.5,\n",
    "                 f'Obsv: {step+1}', transform=axs[step,0].transAxes,\n",
    "                 fontsize=10, fontweight='bold'\n",
    "                )\n",
    "\n",
    "        # Print text to right\n",
    "        if step != n_steps-1:\n",
    "            wrapped_reasoning = textwrap.fill(reasoning[step], wrap_width)\n",
    "            fig.text(0, 1.1, \n",
    "                     f'Action taken: {actions[step]}', transform=axs[step,0].transAxes,\n",
    "                     fontsize=12, ha='left'\n",
    "                    )\n",
    "            fig.text(len(observation_order) + 1.1, 0,\n",
    "                     f'{wrapped_reasoning}',\n",
    "                     transform=axs[step,0].transAxes,\n",
    "                     fontsize=10, ha='left')\n",
    "\n",
    "        # Draw observations\n",
    "        for i in range(len(observation_order)):\n",
    "            axs[step,i].imshow(observations[step][observation_order[i]])\n",
    "            axs[step,i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04013b",
   "metadata": {},
   "source": [
    "# 3. VLM and LLM Model Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea45f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate / \"re-create\" user_prompts as if going through the trajectory for the first time\n",
    "def generate_user_prompt_text(user_prompt_type=None,\n",
    "                              step_num=None,\n",
    "                              action=None,\n",
    "                              inferred_nav_instructions=None,\n",
    "                              navigation_history=[],\n",
    "                              next_step_reasoning=None,\n",
    "                              next_checkpoint_reasoning=None,\n",
    "                              allowable_actions=None\n",
    "                              ):\n",
    "\n",
    "    # Actions available to agent\n",
    "    global view_rotations\n",
    "    if allowable_actions == None:\n",
    "        allowable_actions = [direction.capitalize() for direction in list(view_rotations.keys())]\n",
    "    \n",
    "    # Limiting navigation history to manage memory constraints\n",
    "    earliest_step = max(step_num-20, 0)\n",
    "   \n",
    "    if navigation_history is not None:\n",
    "        latest_step = step_num\n",
    "        if earliest_step == step_num: latest_step = earliest_step+1\n",
    "        latest_navigation_history = '\\n'.join(navigation_history[earliest_step:latest_step])\n",
    "\n",
    "    if user_prompt_type==\"decide_next_checkpoint\":\n",
    "        user_prompt = f\"\"\"The first image is a panoramic image of your current location.\n",
    "        The second image is what you can see directly in front of you.\n",
    "\n",
    "        This is a summary of your progress through the indoor environment from Step {earliest_step} to Step {max(step_num-1, earliest_step)}:\n",
    "        {latest_navigation_history}\n",
    "\n",
    "        Based on that summary and the images provided tell me:\n",
    "        Current Location: Your estimate of your current location\n",
    "        Summary: Summarise the Progress Summary and your current observations\n",
    "        Navigation Checkpoints achieved: Which Navigation Checkpoints have already been achieved. If the Goal checkpoint has been achieved, then state: \"Goal Reached!\"\n",
    "        Navigation Checkpoint to move toward: Which Navigation Checkpoint you should move towards now.\n",
    "        Navigation Checkpoint X Instructions: What the instructions for that Navigation Checkpoint are.\"\"\"\n",
    "\n",
    "    elif user_prompt_type==\"decide_next_step\":\n",
    "        user_prompt = f\"\"\"\n",
    "        This is a summary of which Navigation Checkpoints have already been achieved, and which Navigation Checkpoint you are moving towards:\n",
    "        {next_checkpoint_reasoning}\n",
    "        \n",
    "        Tell me which Navigation Checkpoint you are moving towards.\n",
    "        Tell me which image best aligns with the Navigation Checkpoint you are moving towards and why.\n",
    "        State the direction that the selected image faces from the following options. You can only choose one of these options: {allowable_actions}. You should only choose 'Goal' if you have reached the Goal Checkpoint.\n",
    "        \"\"\"\n",
    "\n",
    "    elif user_prompt_type==\"generate_historical_trajectory\":\n",
    "        user_prompt = f\"\"\"The image is a panoramic image of your current location.\n",
    "        This is a short summary of your trajectory from Step {earliest_step} to Step {step_num-1}:\n",
    "        {latest_navigation_history}\n",
    "\n",
    "        Step {step_num} you decided to take this action: {action.capitalize()}.\n",
    "        You took this action based on this reasoning: {next_step_reasoning}\n",
    "\n",
    "        Write a summary of the action you took in the latest step you took and justify it based on the reasoning provided.\n",
    "        Use the panoramic image of your current location to determine where you are located.\n",
    "        \"\"\"\n",
    "\n",
    "    else:\n",
    "        print(f\"No user_prompt_type provided. Cancelling ... \")\n",
    "        return\n",
    "    \n",
    "    # Removing any indents generated\n",
    "    user_prompt = \"\\n\".join(line.lstrip() for line in user_prompt.splitlines())\n",
    "\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7feed252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the system prompt text\n",
    "# Used in training and testing prompts as well as for the quantization calibration dataset\n",
    "def generate_system_prompt_text(system_prompt_type=None, inferred_nav_instructions=None):\n",
    "    \n",
    "    if system_prompt_type == \"infer_navigation_instructions\":\n",
    "        system_prompt = f\"\"\"I am helping an agent navigate through an indoor environment. I am provided some navigation instructions in natural language.\n",
    "        My task is to convert the instructions into a series of detailed Navigation Checkpoints that the agent must navigate to.\n",
    "        At no point will I need to open any doors or interact with the environment.\n",
    "        \"\"\"\n",
    "    \n",
    "    elif system_prompt_type == \"decide_next_step\":\n",
    "        system_prompt = f\"\"\"I am an agent navigating an indoor environment.\n",
    "\n",
    "        I am provided a series of 6 images.\n",
    "        The first image is what I can see directly forward of me.\n",
    "        The second image is what I can see 45 degrees to my right.\n",
    "        The third image is what I can see 90 degrees to my right.\n",
    "        The fourth image is what I can see 45 degrees to my left.\n",
    "        The fifth image is what I can see 90 degrees to my left.\n",
    "        The sixth image is what I can see directly behind me.\n",
    "\n",
    "        My task is to identify the image direction that best aligns with the Navigation Checkpoint I am working toward.\n",
    "        \"\"\"\n",
    "    \n",
    "    elif system_prompt_type == \"generate_historical_trajectory\":\n",
    "        system_prompt = f\"\"\"I am navigating an indoor environment by following these Navigation Checkpoints: {inferred_nav_instructions}.\n",
    "        My task is to write a summary of the latest step I have taken and explain why I took that step.\n",
    "        \"\"\"\n",
    "\n",
    "    elif system_prompt_type == \"decide_next_checkpoint\":\n",
    "        system_prompt = f\"\"\"I am an agent navigating an indoor environment by a series of navigation checkpoints.\n",
    "        \n",
    "        Navigation Checkpoints:\n",
    "        {inferred_nav_instructions}\n",
    "        \n",
    "        I am provided with a panoramic image of my current position.\n",
    "\n",
    "        I am also provided with a Progress Summary, which contains a record of the steps I have already taken and a list of all the Navigation Checkpoints I have already achieved.\n",
    "        \n",
    "        My task is to use the panoramic image and my Progress Summary to determine:\n",
    "        1. Which Navigation Checkpoints have already been achieved.\n",
    "        2. Which Navigation Checkpoint I should work toward next.\n",
    "        3. What the instructions for that Navigation Checkpoint are.\n",
    "\n",
    "        I must consider the following:\n",
    "        1. I must achieve each Navigation Checkpoint in order.\n",
    "        2. I must verify achievement of each checkpoint by comparing my historical actions and current observations with the instructions for the current Navigation Checkpoint.\n",
    "        3. If a Navigation Checkpoint is achieved, I must move to the next Navigation Checkpoint and determine the required action.\n",
    "        \"\"\"\n",
    "    \n",
    "    else:\n",
    "        print(f\"No system_prompt_type provided. Cancelling ...\")\n",
    "        return\n",
    "\n",
    "    # Removing any indents generated\n",
    "    system_prompt = \"\\n\".join(line.lstrip() for line in system_prompt.splitlines())\n",
    "\n",
    "    return system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a3a2fbf-b90d-4f19-80f0-e91bc67a46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asks the LLM to 'translate' the raw instructions from RxR into a more structured series of checkpoints\n",
    "def infer_navigation_instructions(nav_instructions):\n",
    "    system_prompt = generate_system_prompt_text(system_prompt_type=\"infer_navigation_instructions\")\n",
    "\n",
    "    user_prompt = f\"\"\"Convert the following navigation instructions into a series of steps: {nav_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    assistant_prompt = f\"\"\"You must be as specific as possible. Make sure the Navigation Checkpoints contain exactly one action.\n",
    "    You must follow the structure below. Do not deviate from the template below.\n",
    "    Starting Point: starting position\n",
    "    Checkpoint 1: Perform one action (e.g. turn, walk, move)\n",
    "    Checkpoint 2: Perform one action\n",
    "    Checkpoint 3: Perform one action\n",
    "    ...\n",
    "    Goal:\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\" : \"system\",\n",
    "         \"content\" : system_prompt\n",
    "        },\n",
    "        {\"role\" : \"user\",\n",
    "         \"content\" : user_prompt\n",
    "        },\n",
    "        {\"role\" : \"assistant\",\n",
    "         \"content\" : assistant_prompt\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    #output = generate_outputs_qwen2(model_vlm_1, processor_vlm_1, messages)\n",
    "    output = generate_outputs_openai(llm_client, messages, model=llm_model)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f79bb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_next_checkpoint(inferred_nav_instructions, step_num, panorama, agent_observations, navigation_history):\n",
    "    system_prompt = generate_system_prompt_text(system_prompt_type=\"decide_next_checkpoint\",\n",
    "                                                inferred_nav_instructions=inferred_nav_instructions)\n",
    "\n",
    "    # Limiting navigation history to manage memory constraints\n",
    "    earliest_step = max(step_num-20, 0)\n",
    "    if navigation_history is not None:\n",
    "        latest_step = step_num\n",
    "        if earliest_step == step_num: latest_step + 1\n",
    "        latest_navigation_history = \"\\n\".join(navigation_history[earliest_step:latest_step])\n",
    "    \n",
    "    user_prompt = generate_user_prompt_text(user_prompt_type=\"decide_next_checkpoint\",\n",
    "                                            step_num=step_num,\n",
    "                                            navigation_history=navigation_history)\n",
    "\n",
    "    assistant_prompt = f\"\"\"You must use the following structure to answer. Replace 'X' with the Navigation Checkpoint number you are currently working towards. Do not deviate from this format:\n",
    "    Current Location: Based on the panoramic image of my current location, I can see ... which means that I am currently located in ...\n",
    "    Summary: I started at the Startping Point and began working my way to Checkpoint 1 by ...\n",
    "    Navigation Checkpoints achieved: ...\n",
    "    Navigation Checkpoint to move toward: X\n",
    "    Navigation Checkpoint X Instructions: ...\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\" : [\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(panorama)},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['forward'])},\n",
    "                {\"type\":\"text\", \"text\": user_prompt}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_prompt\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    output = generate_outputs_qwen2(model_vlm_1, processor_vlm_1, messages, max_tokens=256)\n",
    "    output = \"\\n\".join(line.lstrip() for line in output.splitlines())\n",
    "\n",
    "    return {\"user_prompt\": user_prompt, \"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f66a665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts the LLM to make a decision on which action to take next and to explain the \"reasoning\" for taking this action\n",
    "# Aligned with the structure of rationalize_next_step\n",
    "def decide_next_step(agent, step_num, agent_observations, next_checkpoint_reasoning):\n",
    "\n",
    "    # Actions available to agent\n",
    "    action_dict = {\"Forward\" : \"Move Forward\",\n",
    "                   \"Forward-right\" : \"Turn 45 degrees Right\",\n",
    "                   \"Right\" : \"Turn 90 degrees Right\",\n",
    "                   \"Forward-left\" : \"Turn 45 degrees Left\",\n",
    "                   \"Left\" : \"Turn 90 degrees Left\",\n",
    "                   \"Behind\" : \"Turn Around\",\n",
    "                   \"Goal\": \"Goal Reached!!\"}\n",
    "\n",
    "    # Check if moving forwards would result in a collision\n",
    "    # Simulates the presence of a laser distance scanner\n",
    "    allowable_actions = [\"Forward-right\", \"Right\", \"Forward-left\", \"Left\", \"Behind\", \"Goal\"]\n",
    "    forward_is_navigable, _ = check_step_collision(agent=agent)\n",
    "    if forward_is_navigable: allowable_actions = [\"Forward\"] + allowable_actions\n",
    "      \n",
    "    system_prompt = generate_system_prompt_text(system_prompt_type=\"decide_next_step\")\n",
    "\n",
    "    user_prompt = generate_user_prompt_text(user_prompt_type=\"decide_next_step\",\n",
    "                                            step_num=step_num,\n",
    "                                            next_checkpoint_reasoning=next_checkpoint_reasoning,\n",
    "                                            allowable_actions=allowable_actions)\n",
    "\n",
    "    assistant_prompt = f\"\"\"You must use the following structure when answering. 'X' is a placeholder. You must replace 'X' with the actual Navigation Checkpoint number, Image number, or Selected image direction.\n",
    "    Do not deviate from this format:\n",
    "    I am currently working towards Navigation Checkpoint: X\n",
    "    Navigation Checkpoint X instructions are to: ...\n",
    "\n",
    "    Image that best aligns with Navigation Checkpoint X: Image X\n",
    "    Why Image X was selected: Image X (direction) shows ...\n",
    "    Selected image direction: X\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\" : [\n",
    "                #{\"type\":\"image\", \"image\":Image.fromarray(panorama)},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['forward'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['forward-right'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['right'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['forward-left'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['left'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['behind'])},\n",
    "                {\"type\":\"text\", \"text\": user_prompt}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_prompt\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    output = generate_outputs_qwen2(model_vlm_1, processor_vlm_1, messages, max_tokens=256)\n",
    "    output = \"\\n\".join(line.lstrip() for line in output.splitlines())\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18ad7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts the LLM to construct a short historical trajectory of past actions and reasoning\n",
    "def generate_historical_trajectory(inferred_nav_instructions, step_num=0, panorama=None, next_step_reasoning=None, selected_action=None, navigation_history=None):\n",
    "    \n",
    "    # Limiting navigation history to manage memory constraints\n",
    "    earliest_step = max(step_num-20, 0)\n",
    "    if navigation_history is not None:\n",
    "        latest_step = step_num\n",
    "        if earliest_step == step_num: latest_step = earliest_step + 1\n",
    "        latest_navigation_history = \"\\n\".join(navigation_history[earliest_step:latest_step])\n",
    "\n",
    "    if next_step_reasoning is None or panorama is None:\n",
    "        return 'Step 0: I have achieved the checkpoint: Starting Point. No other Navigation Checkpoints have been achieved yet.'\n",
    "\n",
    "    system_prompt = generate_system_prompt_text(system_prompt_type=\"generate_historical_trajectory\",\n",
    "                                                inferred_nav_instructions=inferred_nav_instructions)\n",
    "\n",
    "    user_prompt = generate_user_prompt_text(user_prompt_type=\"generate_historical_trajectory\",\n",
    "                                            step_num=step_num,\n",
    "                                            action=selected_action,\n",
    "                                            navigation_history=navigation_history,\n",
    "                                            next_step_reasoning=next_step_reasoning)\n",
    "\n",
    "    # Translating selected_action into a more verbose form\n",
    "    action_dict = {\"Forward\" : \"Moved Forward\",\n",
    "                   \"Left\" : \"Turned 90 degrees Left\",\n",
    "                   \"Right\" : \"Turned 90 degrees Right\",\n",
    "                   \"Behind\" : \"Turned Around\",\n",
    "                   \"Forward-left\" : \"Turned 45 degrees Left\",\n",
    "                   \"Forward-right\" : \"Turned 45 degrees Right\"}\n",
    "    \n",
    "    try:\n",
    "        selected_direction = action_dict[selected_action]\n",
    "    except:\n",
    "        selected_direction = selected_action\n",
    "\n",
    "    assistant_prompt = f\"\"\"You must use the following as a template for your answer. Replace 'X' with the Navigation Checkpoint number you are currently working towards.\n",
    "    Do not deviate from this template:\n",
    "    Step {step_num}: To work towards Checkpoint X, I took the action: {selected_action.capitalize()}, because ... Based on the panoramic image, I am now located in ....\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\" : \"system\",\n",
    "         \"content\" : system_prompt\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "         \"content\" : [\n",
    "             {\"type\":\"image\", \"image\":Image.fromarray(panorama)},\n",
    "             {\"type\":\"text\", \"text\":user_prompt}\n",
    "         ]\n",
    "        },\n",
    "        {\"role\" : \"assistant\",\n",
    "         \"content\" : assistant_prompt\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    output = generate_outputs_qwen2(model_vlm_1, processor_vlm_1, messages, max_tokens=256)\n",
    "    \n",
    "    # Removing any indents from generated text\n",
    "    output = \"\\n\".join(line.lstrip() for line in output.splitlines())\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f079c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts the LLM to rationalize the action that was identified as the best next step for training\n",
    "# Used only in generating training data\n",
    "def rationalize_next_step(inferred_nav_instructions, agent_observations, navigation_history, next_checkpoint, recommended_action):\n",
    "    \n",
    "    # Translating selected_action into a more verbose form\n",
    "    action_dict = {\"Forward\" : \"Move Forward\",\n",
    "                   \"Forward-right\" : \"Turn 45 degrees Right\",\n",
    "                   \"Right\" : \"Turn 90 degrees Right\",\n",
    "                   \"Forward-left\" : \"Turn 45 degrees Left\",\n",
    "                   \"Left\" : \"Turn 90 degrees Left\",\n",
    "                   \"Behind\" : \"Turn Around\",\n",
    "                   \"Goal\": \"Goal Reached!!\"}\n",
    "    \n",
    "    image_dict = {\"forward\": \"1\",\n",
    "                  \"forward-right\": \"2\",\n",
    "                  \"right\": \"3\",\n",
    "                  \"forward-left\": \"4\",\n",
    "                  \"left\": \"5\",\n",
    "                  \"behind\": \"6\"}\n",
    "    \n",
    "    action = action_dict[recommended_action.capitalize()]\n",
    "\n",
    "    system_prompt = f\"\"\"I am an agent navigating an indoor environment.\n",
    "\n",
    "    I am provided a series of 6 images.\n",
    "    The first image is what I can see directly forward of me.\n",
    "    The second image is what I can see 45 degrees to my right.\n",
    "    The third image is what I can see 90 degrees to my right.\n",
    "    The fourth image is what I can see 45 degrees to my left.\n",
    "    The fifth image is what I can see 90 degrees to my left.\n",
    "    The sixth image is what I can see directly behind me.\n",
    "\n",
    "    My task is to provide the reason why the action to {action} is the one that most aligns with the Navigation Checkpoint I am currently working toward.\n",
    "    \"\"\"\n",
    "    if recommended_action != \"goal\":\n",
    "        user_prompt = f\"\"\"This is a summary of which Navigation Checkpoints have already been achieved, and which Navigation Checkpoint you are moving towards: {next_checkpoint}.\n",
    "\n",
    "        Tell me which Navigation Checkpoint you are moving towards.\n",
    "        Tell me why Image {image_dict[recommended_action]} is the image that best aligns with the Navigation Checkpoint you are moving towards.\n",
    "        State the direction that Image {image_dict[recommended_action]} faces from the following options. You can only choose one of these options: {list(action_dict.keys())}. You should only choose 'Goal' if you have reached the Goal Checkpoint.\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_prompt = f\"\"\"You must use the following structure when answering. Replace 'X' with the Navigation Checkpoint number you are currently working towards.\n",
    "        Do not deviate from this format:\n",
    "        I am currently working towards Navigation Checkpoint: X\n",
    "        Navigation Checkpoint X instructions are to: ...\n",
    "\n",
    "        Image that best aligns with Navigation Checkpoint X: Image {image_dict[recommended_action]}\n",
    "        Why Image {image_dict[recommended_action]} was selected: Image {image_dict[recommended_action]} shows ...\n",
    "        Selected image direction: {recommended_action.capitalize()}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        user_prompt = f\"\"\"This is a summary of which Navigation Checkpoints have already been achieved, and which Navigation Checkpoint you are moving towards: {next_checkpoint}.\n",
    "        \n",
    "        Tell me which Navigation Checkpoint you are moving towards.\n",
    "        Based on what you can see in front of you, tell me why you have reached the Goal.\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_prompt = f\"\"\"You must use the following structure when answering. Replace 'X' with the Navigation Checkpoint number you are currently working towards. Do not deviate from this format:\n",
    "        I am currently working towards Navigation Checkpoint: X\n",
    "        Navigation Checkpoint X instructions are to: ...\n",
    "\n",
    "        Based on what I can see in front of me, which is ... , this matches the description of Goal. This means I have reached the goal.\n",
    "        Goal reached!!\n",
    "        \"\"\"\n",
    "            \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\" : [\n",
    "                #{\"type\":\"image\", \"image\":Image.fromarray(panorama)},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['forward'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['forward-right'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['right'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['forward-left'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['left'])},\n",
    "                {\"type\":\"image\", \"image\":Image.fromarray(agent_observations['behind'])},\n",
    "                {\"type\":\"text\", \"text\": user_prompt}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_prompt\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    output = generate_outputs_qwen2(model_vlm_1, processor_vlm_1, messages, max_tokens=256)\n",
    "\n",
    "    # Removing any indents from generated text\n",
    "    output = \"\\n\".join(line.lstrip() for line in output.splitlines())\n",
    "    \n",
    "    return {\"user_prompt\": user_prompt, \"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "337a5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts the LLM to construct a short historical trajectory of past actions and reasoning for training\n",
    "# Used only in generating training data\n",
    "def generate_training_historical_trajectory(inferred_nav_instructions, step_num=0, panorama=None, reasoning=None, selected_action=None, navigation_history=[]):\n",
    "    \n",
    "    # Limiting navigation history to manage memory constraints\n",
    "    earliest_step = max(step_num-20, 0)\n",
    "    if navigation_history is not None:\n",
    "        latest_step = step_num\n",
    "        if earliest_step == step_num: latest_step + 1\n",
    "        latest_navigation_history = \"\\n\".join(navigation_history[earliest_step:latest_step])\n",
    "\n",
    "    if reasoning is None or panorama is None:\n",
    "        navigation_history = ['Step 0: I have achieved the checkpoint: Starting Point. No other Navigation Checkpoints have been achieved yet.']\n",
    "        return {\"user_prompt\": None,\n",
    "                \"assistant_prompt\": navigation_history[0],\n",
    "                \"navigation_history\": navigation_history}\n",
    "    \n",
    "    system_prompt = generate_system_prompt_text(system_prompt_type=\"generate_historical_trajectory\",\n",
    "                                                inferred_nav_instructions=inferred_nav_instructions)\n",
    "\n",
    "    user_prompt = f\"\"\"The image is a panoramic image of your current location.\n",
    "    This is a short summary of your trajectory from Step {earliest_step} to Step {step_num-1}: {latest_navigation_history}\n",
    "    Step {step_num} you decided to take this action: {selected_action.capitalize()}.\n",
    "    You took this action based on this reasoning: {reasoning}\n",
    "\n",
    "    Write a summary of the action you took in the latest step you took and justify it based on the reasoning provided.\n",
    "    Use the panoramic image of your current location to determine where you are located.\n",
    "    \"\"\"\n",
    "    \n",
    "    assistant_prompt =f\"\"\"You must use the following as a template for your answer. Replace 'X' with the Navigation Checkpoint number you are currently working towards.\n",
    "    Do not deviate from the following template:\n",
    "    Step {step_num}: To work towards Checkpoint X, I took the action: {selected_action.capitalize()} because ... Based on the panoramic image, I am now located in ...\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\" : \"system\",\n",
    "         \"content\" : system_prompt\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "         \"content\" : [\n",
    "             {\"type\":\"image\", \"image\":Image.fromarray(panorama)},\n",
    "             {\"type\":\"text\", \"text\":user_prompt}\n",
    "         ]\n",
    "        },\n",
    "        {\"role\" : \"assistant\",\n",
    "         \"content\" : assistant_prompt\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    output = generate_outputs_qwen2(model_vlm_1, processor_vlm_1, messages, max_tokens=256)\n",
    "    output = \"\\n\".join(line.lstrip() for line in output.splitlines())\n",
    "    navigation_history.append(output)\n",
    "\n",
    "    return {\"user_prompt\": user_prompt, \"output\": output, \"navigation_history\": navigation_history}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eff538",
   "metadata": {},
   "source": [
    "# 4. Generating training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709c345",
   "metadata": {},
   "source": [
    "## 4.1 Generating Training Data: Calculating the \"Gold\" Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4fa33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the gold trajectory / gold path for a single instruction\n",
    "def get_gold_trajectory(agent, scan_id, instruction_id, verbose=0):\n",
    "    outputs = dict()\n",
    "    pose_trace = get_pose_trace(pose_dir_path, instruction_id)\n",
    "    unique_positions_list, unique_rotation_list = get_unique_poses(pose_trace)\n",
    "    \n",
    "    recommended_agent_states = []                   # stores the agent states along the recommended trajectory\n",
    "    recommended_actions = []                        # stores the recommended actions to achieve recommended trajectory\n",
    "    distances = dict()                              # stores the geodesic distance between each direction and the next waypoint\n",
    "    paths = dict()                                  # stores the shortest path from each direction and the next waypoint (used for debug)\n",
    "    current_waypoint = 1                            # the current position along the gold_path we are working toward\n",
    "    last_waypoint = len(unique_positions_list)-1    # the final position on the gold_path (the goal)\n",
    "    reject_trajectory = False                       # determines whether the current trajectory / instruction is suitable for training\n",
    "    step_num = 0\n",
    "\n",
    "    # Setting agent to the starting position in the trajectory\n",
    "    agent_state = habitat_sim.AgentState()\n",
    "    agent_state = snap_to_point(agent_state, unique_positions_list[0] + sensor_height_adjustment , unique_rotation_list[0])\n",
    "    agent.set_state(agent_state)\n",
    "    recommended_agent_states.append(agent.get_state())\n",
    "\n",
    "    while current_waypoint <= last_waypoint and step_num <= max_steps:\n",
    "        if verbose > 0:\n",
    "            print(f'Step: {step_num}')\n",
    "            print(f'Current Waypoint: {current_waypoint} of {last_waypoint}')\n",
    "        \n",
    "        # Retrieving distances to next waypoint from each possible action\n",
    "        # Returns np.inf as the distance for an action if that action results in a non-navigable point\n",
    "        for direction in view_rotations.keys():\n",
    "            is_navigable, potential_position = check_step_collision(agent, view_rotations[direction])\n",
    "            if is_navigable:\n",
    "                dist, path = find_geodesic_distance([potential_position], unique_positions_list[current_waypoint]+sensor_height_adjustment)\n",
    "                distances[direction] = dist[0]\n",
    "                paths[direction] = path\n",
    "            else:\n",
    "                distances[direction] = np.inf\n",
    "        \n",
    "        # Returns the action that yields the lowest distance to the next waypoint\n",
    "        recommended_action = min(distances, key=distances.get)\n",
    "        if verbose > 0:\n",
    "            print(f'Distance: {distances}')\n",
    "            print(f'Recommended action: {recommended_action}')\n",
    "        take_action(agent, recommended_action.capitalize())\n",
    "        recommended_agent_states.append(agent.get_state())\n",
    "        recommended_actions.append(recommended_action)\n",
    "\n",
    "        # Updating step number and waypoint number (if appropriate)\n",
    "        step_num += 1\n",
    "        if distances[recommended_action] <= min_waypoint_distance:\n",
    "            current_waypoint += 1\n",
    "            if verbose > 0:\n",
    "                print(f'Waypoint {current_waypoint-1} achieved.')\n",
    "                print(f'Moving on to waypoint: {current_waypoint}')\n",
    "\n",
    "    # If final distance to goal is greater than threshold, we reject the trajectory for training\n",
    "    #  as the trajectory may require too many steps or not achievable (e.g. requires 'jumping over' obstacles)\n",
    "    distance_to_goal, _ = find_geodesic_distance([recommended_agent_states[-1].position], unique_positions_list[-1]+sensor_height_adjustment)\n",
    "    reject_trajectory = distance_to_goal[0] > min_goal_distance\n",
    "    \n",
    "    # Adding \"goal\" action to trajectory if goal was reached\n",
    "    if not reject_trajectory: recommended_actions.append(\"goal\")\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Final distance to goal: {distance_to_goal}')\n",
    "        print(f'Reject Trajectory?: {reject_trajectory}')\n",
    "    \n",
    "    # Drawing map of environment for debugging / illustration purposes\n",
    "    topdown_map = draw_map(agent, scan_id)\n",
    "    \n",
    "    outputs[\"recommended_agent_states\"] = recommended_agent_states\n",
    "    outputs[\"recommended_actions\"] = recommended_actions\n",
    "    outputs[\"unique_positions_list\"] = unique_positions_list\n",
    "    #outputs[\"topdown_map\"] = topdown_map #(commenting out this datapoint to save on file space)\n",
    "    \n",
    "    return reject_trajectory, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f7a2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the gold trajectories / gold paths for a list of scans\n",
    "def get_eligible_gold_trajectories(scan_id_list, rxr_annotations, verbose=0):\n",
    "    global cfg\n",
    "    global sim\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    # Closing any potentially still open simulator sessions\n",
    "    try:\n",
    "        sim.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for scan_id in scan_id_list:\n",
    "        print(f\"scan_id: {scan_id}\")\n",
    "        update_scan_path(scan_id)\n",
    "        cfg = make_cfg(sim_settings)\n",
    "        sim = habitat_sim.Simulator(cfg)\n",
    "        agent = sim.get_agent(0)\n",
    "\n",
    "        for annotation in rxr_annotations[scan_id]:\n",
    "            instruction_id = annotation[\"instruction_id\"]\n",
    "            \n",
    "            reject_trajectory, trajectory_data = get_gold_trajectory(agent=agent, \n",
    "                                                                     scan_id=scan_id,\n",
    "                                                                     instruction_id=instruction_id,\n",
    "                                                                     verbose=0)\n",
    "\n",
    "            if not reject_trajectory:\n",
    "                trajectory_data[\"scan_id\"] = scan_id\n",
    "                trajectory_data[\"instruction_id\"] = instruction_id\n",
    "                \n",
    "                outputs.append(trajectory_data)\n",
    "            \n",
    "            print(f\"instruction_id {instruction_id} rejected? {reject_trajectory}\")\n",
    "        \n",
    "        sim.close()\n",
    "\n",
    "        # Output format:\n",
    "        # outputs = [{trajectory0}, {trajectory1}, ...]\n",
    "        # trajectory0 = {recommended_agent_states: [agent_state, ...],\n",
    "        #                recommended_actions: [string, ...],\n",
    "        #                unique_positions_list: [np.array, ...],\n",
    "        #                scan_id: string,\n",
    "        #                instruction_id: string}\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01a50df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting RxR training data (annotations / instructions)\n",
    "scan_id_list, rxr_annotations, n_records = get_rxr_annotations(training_annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e8513c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name of gold trajectories (for saving or retrieving from)\n",
    "gold_trajectories_fname = f'eligible_gold_paths_nomap'\n",
    "\n",
    "# Assign to False if gold trajectories are already calculated\n",
    "calculate_gold_trajectories = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f079c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating all gold trajectories in training data\n",
    "if calculate_gold_trajectories:\n",
    "    gold_trajectories = get_eligible_gold_trajectories(scan_id_list, rxr_annotations, verbose=0)\n",
    "    random.shuffle(gold_trajectories)   # Shuffling to avoid 'chunks' of the same scan occuring in the data\n",
    "    save_results(gold_trajectories, training_dir_path, gold_trajectories_fname)\n",
    "else:\n",
    "    print(f'No recalculation of gold trajectories. Use existing training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe081db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving eligible gold paths to generate data from\n",
    "training_file_path = training_dir_path + gold_trajectories_fname + '.pkl'\n",
    "eligible_gold_trajectories = load_results(training_file_path)\n",
    "\n",
    "print(f\"Number of eligible trajectories: {len(eligible_gold_trajectories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2c143",
   "metadata": {},
   "source": [
    "## 4.2 Generating Training Data: Generating Trajectory Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2bedc6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vlm():\n",
    "    global model_vlm_1\n",
    "    global processor_vlm_1\n",
    "\n",
    "    try:\n",
    "        model_vlm_1.to('cpu')\n",
    "        del model_vlm_1\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    model_vlm_1 = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    processor_vlm_1 = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29ec3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data for one trajectory (i.e. one instruction_id)\n",
    "def generate_training_trajectory_data(scan_id, instruction_id, rxr_annotations, recommended_actions):\n",
    "    global view_rotations\n",
    "    global cfg\n",
    "    global sim\n",
    "\n",
    "    trajectory_data = []\n",
    "\n",
    "    try:\n",
    "        sim.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Initializing variables used to track agent's logic / reasoning\n",
    "    next_step_reasoning = None\n",
    "    navigation_history = []\n",
    "\n",
    "    # Creating simulator environment\n",
    "    update_scan_path(scan_id)\n",
    "    cfg = make_cfg(sim_settings)\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "    agent = sim.get_agent(0)\n",
    "\n",
    "    # Retrieving pose trace of requested instruction_id\n",
    "    pose_trace = get_pose_trace(pose_dir_path, instruction_id)\n",
    "    unique_position_list, unique_rotation_list = get_unique_poses(pose_trace)\n",
    "\n",
    "    # Setting agent to the starting position in the trajectory\n",
    "    agent_state = habitat_sim.AgentState()\n",
    "    agent_state = snap_to_point(agent_state, unique_position_list[0] + sensor_height_adjustment , unique_rotation_list[0])\n",
    "    agent.set_state(agent_state)\n",
    "\n",
    "    # Get inferred navigation checkpoints\n",
    "    nav_instructions_list = rxr_annotations[scan_id]\n",
    "    for instructions in nav_instructions_list:\n",
    "        if instructions[\"instruction_id\"] == instruction_id:\n",
    "            nav_instructions = instructions[\"instruction\"]\n",
    "    \n",
    "    inferred_nav_instructions = infer_navigation_instructions(nav_instructions)\n",
    "\n",
    "    print(f\"scan_id: {scan_id}\")\n",
    "    print(f\"instruction_id: {instruction_id}\")\n",
    "    # Retrieve the observations\n",
    "    for i, action in enumerate(recommended_actions):\n",
    "        print(f\"Step: {i} of {len(recommended_actions)}: {action}\")\n",
    "              \n",
    "        step_data = dict()\n",
    "        \n",
    "        agent_views, _ = get_current_observations(agent, view_rotations)\n",
    "        current_panorama = get_current_panorama(agent)\n",
    "\n",
    "        # Put each function in a try-except block to handle CUDA Out of Memory errors\n",
    "        # We recreate / restart the VLM in each error case to help \"reset\" memory usage\n",
    "        print(f\"\\tCreating nav_history_data\")\n",
    "        try:\n",
    "            nav_history_data = generate_training_historical_trajectory(inferred_nav_instructions=inferred_nav_instructions,\n",
    "                                                                        step_num=i,\n",
    "                                                                        panorama=current_panorama,\n",
    "                                                                        reasoning=next_step_reasoning,\n",
    "                                                                        selected_action=recommended_actions[i-1],             # action from prior step\n",
    "                                                                        navigation_history=navigation_history)\n",
    "            navigation_history = nav_history_data[\"navigation_history\"]\n",
    "        except:\n",
    "            nav_history_data = [f\"Error on step {i}: nav_history_data\"]\n",
    "            print(f\"Error on step: {i}: nav_history_data\")\n",
    "            print(f\"nav_history_data:\\n{nav_history_data}\")\n",
    "            create_vlm()\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\tCreating reasoning_next_checkpoint\")\n",
    "        try:\n",
    "            reasoning_next_checkpoint = decide_next_checkpoint(inferred_nav_instructions = inferred_nav_instructions,\n",
    "                                                                step_num=i,\n",
    "                                                                panorama = current_panorama,\n",
    "                                                                agent_observations = agent_views,\n",
    "                                                                navigation_history = navigation_history)\n",
    "            next_checkpoint_reasoning = reasoning_next_checkpoint[\"output\"]\n",
    "            \n",
    "        except:\n",
    "            reasoning_next_checkpoint = [f\"Error on step {i}: reasoning_next_checkpoint\"]\n",
    "            print(f\"Error on step: {i}: reasoning_next_checkpoint\")\n",
    "            print(f\"reasoning_next_checkpoint:\\n{next_checkpoint_reasoning}\")\n",
    "            create_vlm()\n",
    "            continue\n",
    "\n",
    "        print(f\"\\tCreating next_step_rationale\")\n",
    "        try:\n",
    "            next_step_rationale = rationalize_next_step(inferred_nav_instructions = inferred_nav_instructions,\n",
    "                                                        agent_observations = agent_views,\n",
    "                                                        navigation_history = navigation_history,\n",
    "                                                        next_checkpoint = next_checkpoint_reasoning,\n",
    "                                                        recommended_action = action)\n",
    "            next_step_reasoning = next_step_rationale[\"output\"]\n",
    "        except:\n",
    "            next_step_rationale = [f\"Error on step {i}: rationalize_next_step\"]\n",
    "            print(f\"Error on step: {i}: rationalize_next_step\")\n",
    "            print(f\"rationalize_next_step:\\n{next_step_reasoning}\")\n",
    "            create_vlm()\n",
    "            continue\n",
    "        \n",
    "        if action == \"goal\":\n",
    "            print(f\"Goal step reached.\")\n",
    "        else:       \n",
    "            take_action(agent, action.capitalize())\n",
    "\n",
    "        print(f\"\\tStep complete.\")\n",
    "\n",
    "        # Storing relevant data from current step into dictionary\n",
    "        step_data[\"observations\"] = agent_views\n",
    "        step_data[\"action\"] = action\n",
    "        step_data[\"next_checkpoint_reasoning\"] = reasoning_next_checkpoint\n",
    "        step_data[\"navigation_history\"] = nav_history_data\n",
    "        step_data[\"next_step_reasoning\"] = next_step_rationale\n",
    "        step_data[\"panorama\"] = current_panorama\n",
    "        trajectory_data.append(step_data)\n",
    "\n",
    "    return inferred_nav_instructions, trajectory_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79047f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(training_trajectories, rxr_annotations, results_fname):\n",
    "    outputs = []\n",
    "\n",
    "    global training_dir_path\n",
    "    \n",
    "    for i, trajectory in enumerate(training_trajectories):\n",
    "\n",
    "        print(f\"Generating data for trajectory {i+1} of {len(training_trajectories)}\")\n",
    "        trajectory_data = dict()\n",
    "\n",
    "        scan_id = trajectory[\"scan_id\"]\n",
    "        instruction_id = trajectory[\"instruction_id\"]\n",
    "        recommended_actions = trajectory[\"recommended_actions\"]\n",
    "        \n",
    "        trajectory_data[\"scan_id\"] = scan_id\n",
    "        trajectory_data[\"instruction_id\"] = instruction_id\n",
    "\n",
    "        trajectory_data[\"inferred_nav_instructions\"], \\\n",
    "        trajectory_data[\"training_data\"] = generate_training_trajectory_data(scan_id=scan_id,\n",
    "                                                                             instruction_id=instruction_id,\n",
    "                                                                             rxr_annotations=rxr_annotations,\n",
    "                                                                             recommended_actions=recommended_actions)\n",
    "        outputs.append(trajectory_data)\n",
    "\n",
    "        # Saving every 5 trajectories so we don't lose everything in case of error\n",
    "        if i%5 == 0:\n",
    "            save_results(outputs, training_dir_path, results_fname)\n",
    "    \n",
    "    # One final save for completed trajectories\n",
    "    save_results(outputs, training_dir_path, results_fname)\n",
    "    \n",
    "    # Output structure:\n",
    "    #   outputs = [{trajectory0}, {trajectory1}, ...]\n",
    "    #   trajectory0 = {scan_id: string,\n",
    "    #                   instruction_id: int,\n",
    "    #                   inferred_nav_instructions: string,\n",
    "    #                   training_data: []}\n",
    "    #   training_data = [step0, step1, ...]\n",
    "    #   step0 = {observations: np.array of color observations,\n",
    "    #            action: string,\n",
    "    #            next_checkpoint_reasoning: {user_prompt: string, assistant_prompt: string},\n",
    "    #            navigation_history: {user_prompt: string, assistant_prompt: string, navigation_history: [string]},\n",
    "    #            next_step_reasoning: {user_prompt: string, assistant_prompt: string},\n",
    "    #            panorama: np.array of panoramic observation}\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7440471",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_fname = 'generated_training_trajectories'\n",
    "generate_new_training_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a85df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_new_training_data:\n",
    "    # Declaring VLM model: Qwen2-VL-7B-Instruct\n",
    "    model_vlm_1 = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    processor_vlm_1 = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "    # Declaring LLM API client: OpenAI\n",
    "    llm_client = OpenAI()\n",
    "    llm_model = \"gpt-4o\"\n",
    "    \n",
    "    # Generating action reasoning, navigation history, checkpoint reasoning from gold trajectories\n",
    "    max_trajectories = int(len(eligible_gold_trajectories)/10)      # Running 10% of eligible trajectories to limit model run-time\n",
    "    max_trajectories = 100\n",
    "\n",
    "    sampled_eligible_gold_trajectories = random.sample(eligible_gold_trajectories,\n",
    "                                                       max_trajectories)\n",
    "\n",
    "    training_data = generate_training_data(training_trajectories=eligible_gold_trajectories,\n",
    "                                        rxr_annotations=rxr_annotations,\n",
    "                                        results_fname=training_data_fname)\n",
    "    print(f'Training data generated.')\n",
    "else:\n",
    "    print(f'No generation of training trajectories conducted. Use existing training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe858e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving generated training trajectories\n",
    "training_trajectories_fpath = training_dir_path + training_data_fname + '.pkl'\n",
    "training_data = load_results(training_trajectories_fpath)\n",
    "\n",
    "if training_data is not None:\n",
    "    print(f\"Number of eligible trajectories: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up local model to free up memory\n",
    "try:\n",
    "    model_vlm_1.to('cpu')\n",
    "    del model_vlm_1\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"VLM garbage collected\")\n",
    "except:\n",
    "    print(f\"VLM instance did not exist. Moving on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3fd5c",
   "metadata": {},
   "source": [
    "# 5. Fine-Tuning VLM\n",
    "The VLM (Qwen2-VL-7B-Instruct) will be finetuned using Low-Rank Adaptation (LoRA), which is a Parameter-Efficient Fine-Tuning (PEFT) technique. LoRA enables the model to be fine-tuned without the need for fine-tuning the entire model, whilst retaining similar performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf21053",
   "metadata": {},
   "source": [
    "## 5.1 Fine-Tuning VLM: Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e213291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_images(image_folder_path, scan_id, instruction_id, step_num, images, image_descriptions):\n",
    "    image_fpaths = dict()\n",
    "    file_extension = '.jpg'\n",
    "    \n",
    "    if not isinstance(images, list):\n",
    "        images = [images]\n",
    "        \n",
    "    if not isinstance(image_descriptions, list):\n",
    "        image_descriptions = [image_descriptions]\n",
    "\n",
    "    if len(images) != len(image_descriptions):\n",
    "        print(f'Error: Number of Images and Image Descriptions do not match.')\n",
    "        return\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        description = image_descriptions[i]\n",
    "        image_fname = f'{scan_id}-{instruction_id}_step{step_num}_{description}'\n",
    "        image_fpath = image_folder_path + image_fname + file_extension\n",
    "       \n",
    "        # Saving it in RGB rather than RGBA\n",
    "        picture = Image.fromarray(image[:,:,:3]) # 4th channel is Alpha\n",
    "        picture.save(image_fpath)\n",
    "        \n",
    "        image_fpaths[description] = image_fpath\n",
    "    \n",
    "    return image_fpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b09fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a user prompt, assistant prompt, and some images and returns it in the sharegpt dataset format\n",
    "# Assumes images will be passed as numpy arrays\n",
    "def create_training_datapoint(user_prompt, assistant_prompt, image_fpath):\n",
    "    datapoint = dict()\n",
    "\n",
    "    messages = [\n",
    "        {\"content\": user_prompt,\n",
    "         \"role\": \"user\"},\n",
    "         {\"content\": assistant_prompt,\n",
    "          \"role\": \"assistant\"}\n",
    "    ]\n",
    "\n",
    "    if not isinstance(image_fpath, list):\n",
    "        image_fpath = [image_fpath]\n",
    "    \n",
    "    datapoint[\"messages\"] = messages\n",
    "    datapoint[\"images\"] = image_fpath\n",
    "\n",
    "    return datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d34bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataset of training data from the generated training trajectories\n",
    "# Each step will provide 3 datapoints: navigation history, checkpoint reasoning, step / action reasoning\n",
    "def create_training_dataset(data):\n",
    "    global training_dir_path\n",
    "    dataset = []\n",
    "\n",
    "    images_folder = training_dir_path + 'finetuning_images/'\n",
    "\n",
    "    print(f'Total number of trajectories: {len(data)}')\n",
    "    for i, trajectory in enumerate(data):\n",
    "        trajectory_data = trajectory[\"training_data\"]\n",
    "        scan_id = trajectory[\"scan_id\"]\n",
    "        instruction_id = trajectory[\"instruction_id\"]\n",
    "        \n",
    "        print(f'Trajectory {i+1} of {len(data)}: {len(trajectory_data)} steps')\n",
    "        \n",
    "        # For each step in the trajectory, create a set of training data for each generated reasoning\n",
    "        for step_num, step_data in enumerate(trajectory_data):\n",
    "            image_fpaths = dict()\n",
    "            navigation_history = step_data[\"navigation_history\"]\n",
    "            next_checkpoint_reasoning = step_data[\"next_checkpoint_reasoning\"]\n",
    "            next_step_reasoning = step_data[\"next_step_reasoning\"]\n",
    "\n",
    "            # Checking to see if any of the datapoints were errors\n",
    "            # generate_training_trajectory_data populates the variable with a string in cases of error\n",
    "            if not all(isinstance(variable, dict) for variable in \n",
    "                       (navigation_history, next_checkpoint_reasoning, next_step_reasoning)):\n",
    "                print(f\"Step {step_num+1} includes error data. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Saving images into folder and getting file paths\n",
    "            images_to_save = []\n",
    "            descriptions = [\"panorama\"]\n",
    "\n",
    "            images_to_save.append(step_data[\"panorama\"])\n",
    "            for description, image in step_data[\"observations\"].items():\n",
    "                descriptions.append(description)\n",
    "                images_to_save.append(image)\n",
    "\n",
    "            image_fpaths = save_training_images(image_folder_path=images_folder,\n",
    "                                                scan_id=scan_id,\n",
    "                                                instruction_id=instruction_id,\n",
    "                                                step_num=step_num,\n",
    "                                                images=images_to_save,\n",
    "                                                image_descriptions=descriptions)\n",
    "            \n",
    "            # Navigation History\n",
    "            user_prompt = generate_user_prompt_text(user_prompt_type=\"generate_historical_trajectory\",\n",
    "                                                    step_num=step_num,\n",
    "                                                    action=step_data[\"action\"],\n",
    "                                                    next_step_reasoning=next_checkpoint_reasoning[\"output\"],\n",
    "                                                    navigation_history=navigation_history[\"navigation_history\"])\n",
    "            \n",
    "            try:\n",
    "                assistant_prompt = navigation_history[\"assistant_prompt\"]\n",
    "            except:\n",
    "                assistant_prompt = navigation_history[\"output\"]\n",
    "\n",
    "            assistant_prompt = \"\\n\".join(line.lstrip() for line in assistant_prompt.splitlines())\n",
    "\n",
    "            nav_history_data = create_training_datapoint(user_prompt=\"<image>\" + user_prompt,\n",
    "                                                            assistant_prompt=assistant_prompt,\n",
    "                                                            image_fpath=image_fpaths[\"panorama\"])\n",
    "            dataset.append(nav_history_data)\n",
    "\n",
    "            # Reasons for 'choosing' a specific checkpoint\n",
    "            user_prompt = generate_user_prompt_text(user_prompt_type=\"decide_next_checkpoint\",\n",
    "                                                    step_num=step_num,\n",
    "                                                    navigation_history=navigation_history[\"navigation_history\"])\n",
    "\n",
    "            assistant_prompt = next_checkpoint_reasoning[\"output\"]\n",
    "            assistant_prompt = \"\\n\".join(line.lstrip() for line in assistant_prompt.splitlines())\n",
    "\n",
    "            checkpoint_reasoning_data = create_training_datapoint(user_prompt=\"<image>\"*2 + user_prompt,\n",
    "                                                                  assistant_prompt=assistant_prompt,\n",
    "                                                                  image_fpath=[\n",
    "                                                                      image_fpaths[\"panorama\"],\n",
    "                                                                      image_fpaths[\"forward\"]\n",
    "                                                                  ])\n",
    "            dataset.append(checkpoint_reasoning_data)\n",
    "\n",
    "            # Reasons for 'choosing' to take a specific step\n",
    "            user_prompt = generate_user_prompt_text(user_prompt_type=\"decide_next_step\",\n",
    "                                                    step_num=step_num,\n",
    "                                                    navigation_history=step_data[\"navigation_history\"][\"navigation_history\"])\n",
    "\n",
    "            assistant_prompt = step_data[\"next_step_reasoning\"][\"output\"]\n",
    "            assistant_prompt = \"\\n\".join(line.lstrip() for line in assistant_prompt.splitlines())\n",
    "\n",
    "            next_step_data = create_training_datapoint(user_prompt=\"<image>\"*6 + user_prompt,\n",
    "                                                       assistant_prompt=assistant_prompt,\n",
    "                                                       image_fpath=[\n",
    "                                                           image_fpaths[\"forward\"],\n",
    "                                                           image_fpaths[\"forward-right\"],\n",
    "                                                           image_fpaths[\"right\"],\n",
    "                                                           image_fpaths[\"forward-left\"],\n",
    "                                                           image_fpaths[\"left\"],\n",
    "                                                           image_fpaths[\"behind\"]\n",
    "                                                       ])\n",
    "            dataset.append(next_step_data)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_data = True\n",
    "\n",
    "# Saving data as json file\n",
    "dataset_fname = 'finetuning_dataset.json'\n",
    "dataset_fpath = training_dir_path + dataset_fname\n",
    "\n",
    "if create_training_data:\n",
    "    dataset = create_training_dataset(training_data)\n",
    "    print(f\"Training dataset created\")\n",
    "    \n",
    "    with open(dataset_fpath, 'w', encoding='utf-8') as file:\n",
    "        json.dump(dataset, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Training dataset file saved: {dataset_fpath}\")\n",
    "\n",
    "elif os.path.exists(dataset_fpath):\n",
    "    print(f\"Training dataset not created. Using existing dataset: {dataset_fpath}\")\n",
    "    with open(dataset_fpath) as file:\n",
    "        dataset = json.load(file)\n",
    "else:\n",
    "    print(f\"No training dataset created. No existing dataset found at this path: {dataset_fpath}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = 5000\n",
    "max_len_idx = None\n",
    "idx_to_remove = []\n",
    "\n",
    "for i, datapoint in enumerate(dataset):\n",
    "    if len(datapoint[\"messages\"][0][\"content\"])> max_len:\n",
    "        idx_to_remove.append(i)\n",
    "\n",
    "print(f\"Number of prompts to remove: {len(idx_to_remove)}\")\n",
    "dataset = [x for i, x in enumerate(dataset) if i not in idx_to_remove]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "daad63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_fpath, 'w', encoding='utf-8') as file:\n",
    "        json.dump(dataset, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9144203",
   "metadata": {},
   "source": [
    "## 5.2 Fine-Tuning VLM: LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c87406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and finetuning adapter weights\n",
    "finetune_model = False\n",
    "model_path = \"/home/wes/Documents/Qwen2-VL-7B_VLN-LoRA\"\n",
    "if finetune_model:\n",
    "    !llamafactory-cli train /home/wes/Documents/Qwen2-VL-7B_VLN-LoRA/qwen2vl_lora_sft_vln.yaml\n",
    "else:\n",
    "    print(f\"Use existing model: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging new weights\n",
    "finetune_model = True\n",
    "model_path = \"/home/wes/Documents/Qwen2-VL-7B_VLN-LoRA-merged/\"\n",
    "if finetune_model:\n",
    "    !llamafactory-cli export /home/wes/Documents/Qwen2-VL-7B_VLN-LoRA-merged/qwen2vl_lora_sft_vln_merge.yaml\n",
    "    print(f\"Weighted merged!\")\n",
    "else:\n",
    "    print(f\"Use existing model: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b38dad7",
   "metadata": {},
   "source": [
    "## 5.3 Fine-Tuning VLM: Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61217714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating quantization calibration dataset\n",
    "image_folder_path = training_dir_path + \"calibration_images/\"\n",
    "\n",
    "def generate_calibration_data(dataset=None, num_datapoints=5):\n",
    "    messages = []\n",
    "    data_type = iter([\"generate_historical_trajectory\",\n",
    "                    \"decide_next_checkpoint\",\n",
    "                    \"decide_next_step\"])\n",
    "    data_type = cycle(data_type)\n",
    "\n",
    "    for i in range(num_datapoints):\n",
    "        print(f\"Processing datapoint {i+1} of {num_datapoints}\")\n",
    "        trajectory_data = random.sample(dataset,1)[0]           # Getting a random trajectory\n",
    "        step_num = random.randint(0, len(trajectory_data))      # Getting a random step the trajectory\n",
    "        step_data = trajectory_data[\"training_data\"][step_num]\n",
    "\n",
    "        inferred_nav_instructions = trajectory_data[\"inferred_nav_instructions\"]\n",
    "        scan_id = trajectory_data[\"scan_id\"]\n",
    "        instruction_id = trajectory_data[\"instruction_id\"]\n",
    "\n",
    "        next_step_reasoning = step_data[\"next_step_reasoning\"][\"output\"]\n",
    "        action = step_data[\"action\"]\n",
    "        next_checkpoint_reasoning = step_data[\"next_checkpoint_reasoning\"][\"output\"]\n",
    "        navigation_history = step_data[\"navigation_history\"][\"navigation_history\"]\n",
    "        observations = step_data[\"observations\"]\n",
    "\n",
    "        # Saving training images\n",
    "        images_to_save = []\n",
    "        descriptions = [\"panorama\"]\n",
    "        images_to_save.append(step_data[\"panorama\"])\n",
    "\n",
    "        for description, image in step_data[\"observations\"].items():\n",
    "            descriptions.append(description)\n",
    "            images_to_save.append(image)\n",
    "\n",
    "        image_fpaths = save_training_images(image_folder_path=image_folder_path,\n",
    "                                            scan_id=scan_id,\n",
    "                                            instruction_id=instruction_id,\n",
    "                                            step_num=step_num,\n",
    "                                            images=images_to_save,\n",
    "                                            image_descriptions=descriptions\n",
    "                                            )\n",
    "\n",
    "        # Creating the prompts / messages\n",
    "        datapoint_type = next(data_type)\n",
    "\n",
    "        system_prompt = generate_system_prompt_text(system_prompt_type=datapoint_type,\n",
    "                                                    inferred_nav_instructions=inferred_nav_instructions)\n",
    "        \n",
    "        user_prompt = generate_user_prompt_text(user_prompt_type=datapoint_type,\n",
    "                                                step_num=step_num,\n",
    "                                                action=action,\n",
    "                                                navigation_history=navigation_history,\n",
    "                                                next_step_reasoning=next_step_reasoning,\n",
    "                                                next_checkpoint_reasoning=next_checkpoint_reasoning\n",
    "                                            )\n",
    "\n",
    "        if datapoint_type == \"generate_historical_trajectory\":\n",
    "            assistant_prompt = navigation_history[step_num]\n",
    "            \n",
    "            user_message = {\"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"image\", \"image\":Image.fromarray(step_data[\"panorama\"])},\n",
    "                                {\"type\": \"text\", \"text\": user_prompt}\n",
    "                            ]}\n",
    "        \n",
    "        elif datapoint_type == \"decide_next_checkpoint\":\n",
    "            assistant_prompt = next_checkpoint_reasoning\n",
    "\n",
    "            user_message = {\"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"image\", \"image\": Image.fromarray(step_data[\"panorama\"])},\n",
    "                                {\"type\": \"image\", \"image\": Image.fromarray(observations[\"forward\"])},\n",
    "                                {\"type\": \"text\", \"text\": user_prompt}\n",
    "                            ]}\n",
    "        \n",
    "        elif datapoint_type == \"decide_next_step\":\n",
    "            assistant_prompt = next_step_reasoning\n",
    "\n",
    "            user_message = {\"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"image\", \"image\":Image.fromarray(observations[\"forward\"])},\n",
    "                                {\"type\": \"image\", \"image\":Image.fromarray(observations[\"forward-right\"])},\n",
    "                                {\"type\": \"image\", \"image\":Image.fromarray(observations[\"right\"])},\n",
    "                                {\"type\": \"image\", \"image\":Image.fromarray(observations[\"forward-left\"])},\n",
    "                                {\"type\": \"image\", \"image\":Image.fromarray(observations[\"left\"])},\n",
    "                                {\"type\": \"image\", \"image\":Image.fromarray(observations[\"behind\"])},\n",
    "                                {\"type\": \"text\", \"text\": user_prompt}\n",
    "                            ]}\n",
    "        \n",
    "        else:\n",
    "            print(f\"Something went wrong - could not find data for: {datapoint_type}\")\n",
    "            continue     \n",
    "            \n",
    "        system_message = {\"role\": \"system\",\n",
    "                        \"content\": system_prompt}\n",
    "        assistant_message = {\"role\": \"assistant\",\n",
    "                            \"content\": assistant_prompt}\n",
    "        \n",
    "        messages.append([system_message, user_message, assistant_message,])\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680728fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_calibration_data = False\n",
    "\n",
    "dataset_fname = 'calibration_dataset'\n",
    "dataset_fpath = training_dir_path + dataset_fname\n",
    "\n",
    "if create_calibration_data:\n",
    "    messages = generate_calibration_data(dataset=training_data, num_datapoints=20)\n",
    "    save_results(messages, training_dir_path, dataset_fname)\n",
    "    print(f\"Calibration dataset created: {dataset_fpath}\")\n",
    "elif os.path.exists(dataset_fpath+'.pkl'):\n",
    "    messages = load_results(dataset_fpath+'.pkl', 'rb')\n",
    "    print(f\"Calibration dataset not created. Using existing dataset: {dataset_fpath}\")\n",
    "else:\n",
    "    print(f\"No calibration datatset created. No existing dataset found at this path: {dataset_fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_model = False\n",
    "\n",
    "model_path = \"/home/wes/Documents/Qwen2-VL-7B_VLN-LoRA-merged/\"\n",
    "quant_path = \"/home/wes/Documents/git_ws/Qwen2-VL-7B_VLN-AWQ-4bit/\"\n",
    "\n",
    "if quantize_model:\n",
    "    quant_config = {\n",
    "        \"zero_point\": True,\n",
    "        \"q_group_size\": 128,\n",
    "        \"w_bit\": 4,\n",
    "        \"version\": \"GEMM\"\n",
    "    }\n",
    "    quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "\n",
    "    processor = Qwen2VLProcessor.from_pretrained(model_path)\n",
    "    model = Qwen2VLAWQForConditionalGeneration.from_pretrained(\n",
    "        model_path=model_path,\n",
    "        model_type=\"qwen2_vl\",\n",
    "        use_cache=False,\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    model.quantize(calib_data=inputs, quant_config=quant_config)\n",
    "    print(f\"Model Quantized!\")\n",
    "\n",
    "    model.model.config.use_cache = model.model.generation_config.use_cache = True\n",
    "    model.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\n",
    "    processor.save_pretrained(quant_path)\n",
    "\n",
    "    print(f\"Model saved: {quant_path}\")\n",
    "else:\n",
    "    print(f\"Model not quantized. Use existing quantized model: {quant_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b260da-9242-457d-9cf3-d89d61ad3c12",
   "metadata": {},
   "source": [
    "# 6. Navigating with Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68159e",
   "metadata": {},
   "source": [
    "## 6.1 Navigating with Test Data: Functions to govern testing iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "655bd5b7-868f-48b5-9e1f-b1e7ecdcd249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempts to navigate the agent through a given environment and single set of instructions\n",
    "def navigate_environment(agent, scan_id, pose_trace, nav_instructions, max_actions=10, verbose=0, show_observations=0):\n",
    "    global view_rotations\n",
    "\n",
    "    agent_state = habitat_sim.AgentState(0)\n",
    "    unique_position_list, unique_rotation_list = get_unique_poses(pose_trace)\n",
    "\n",
    "    # Regex pattern to add robustness to action selection\n",
    "    pattern = r\"\\nSelected image direction:\\s*([A-Za-z]+-*[A-Za-z]+)\"\n",
    "    \n",
    "    # Snapping agent to starting position of instruction\n",
    "    agent_state = snap_to_point(agent_state, unique_position_list[0] + sensor_height_adjustment, unique_rotation_list[0])\n",
    "    agent.set_state(agent_state)\n",
    "\n",
    "    if show_observations == 1:\n",
    "        # show starting position panorama - forward view is in centre\n",
    "        pano = get_current_panorama(agent)\n",
    "        plt.imshow(pano)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return Inferred Navigational Checkpoints\n",
    "    inferred_nav_instructions = infer_navigation_instructions(nav_instructions)\n",
    "\n",
    "    if verbose >= 2:\n",
    "        print(f'-------------------------------------')\n",
    "        print(f\"Original navigation instructions:\\n{nav_instructions}\")\n",
    "        print()\n",
    "        print(f\"Inferred Navigation Checkpoints:\\n{inferred_nav_instructions}\")\n",
    "        print()\n",
    "    \n",
    "    # Resetting tracking variables\n",
    "    step_observations = []\n",
    "    step_panoramas = []\n",
    "    past_actions = []\n",
    "    next_step_reasoning = None\n",
    "    step_reasoning_history = []\n",
    "    selected_action = None\n",
    "    navigation_history = []\n",
    "    agent_states = [agent.get_state()] # Starting location\n",
    "\n",
    "    # Taking steps\n",
    "    for step in range(max_actions):\n",
    "        if verbose > 0: print(f\"Step: {step+1}\")\n",
    "\n",
    "        # Generate panoramic view of current step\n",
    "        pano = get_current_panorama(agent)\n",
    "        step_panoramas.append(pano)\n",
    "\n",
    "        # Starting with the past action history to help ground the model\n",
    "        #  and orientate it in navigation checkpoint progress\n",
    "\n",
    "        latest_step_history = generate_historical_trajectory(inferred_nav_instructions=inferred_nav_instructions,\n",
    "                                                             step_num=step,\n",
    "                                                             panorama=pano,\n",
    "                                                             next_step_reasoning=next_step_reasoning,\n",
    "                                                             selected_action=selected_action,\n",
    "                                                             navigation_history=navigation_history)\n",
    "        navigation_history.append(latest_step_history)\n",
    "        \n",
    "\n",
    "        # Getting observations from current location\n",
    "        agent_observations, _ = get_current_observations(agent, view_rotations)\n",
    "        step_observations.append({key:value[:] for key, value in agent_observations.items()}) # Saving colour images to display to user\n",
    "\n",
    "        # Determining which navigation checkpoint is next\n",
    "        next_checkpoint_reasoning = decide_next_checkpoint(inferred_nav_instructions=inferred_nav_instructions,\n",
    "                                                           step_num=step,\n",
    "                                                           panorama=pano,\n",
    "                                                           agent_observations=agent_observations,\n",
    "                                                           navigation_history=navigation_history)[\"output\"]\n",
    "\n",
    "        # Deciding on an action to take based on observations\n",
    "        next_step_reasoning = decide_next_step(agent=agent,\n",
    "                                               step_num=step,\n",
    "                                               agent_observations=agent_observations,\n",
    "                                               next_checkpoint_reasoning=next_checkpoint_reasoning)\n",
    "        match = re.search(pattern, next_step_reasoning)\n",
    "        if match:\n",
    "            # Using regex pattern matching\n",
    "            selected_action = match.group(1).strip()\n",
    "        else:\n",
    "            # Basic character search\n",
    "            selected_action = next_step_reasoning[next_step_reasoning.rfind('Selected image direction:') + len('Selected image direction: '):]\n",
    "\n",
    "        # Check if it believes the goal has been reached\n",
    "        if selected_action == \"Goal Reached\":\n",
    "            print(f\" ---->> Goal has been reached! Ending this trajectory\")\n",
    "            break\n",
    "        \n",
    "        # Take an action\n",
    "        try:\n",
    "            take_action(agent, selected_action)\n",
    "        except:\n",
    "            print(f'Invalid action ({selected_action})')\n",
    "            selected_action = f'Invalid action: {selected_action}'\n",
    "    \n",
    "        # Saving actions taken and positions visited to track agent progress throughout trajectory\n",
    "        past_actions.append(selected_action)\n",
    "        agent_states.append(agent.get_state())\n",
    "\n",
    "        # Displaying results to user depending on verbosity settings\n",
    "        if verbose >= 2:\n",
    "            print(f'navigation_history:\\n{navigation_history}')\n",
    "            print()\n",
    "            print(f'decide_next_checkpoint:\\n{next_checkpoint_reasoning}')           \n",
    "            print(f'Action Reasoning ----\\n{next_step_reasoning}')\n",
    "            print(f'Taking action: {selected_action}')\n",
    "            print(f'Current position: {agent_states[-1].position}')\n",
    "\n",
    "            print(f'-------------------')\n",
    "\n",
    "        step_reasoning_history.append(next_step_reasoning)\n",
    "        \n",
    "    # Retrieving observations one more time to get observations from ending position\n",
    "    agent_observations, color_observations = get_current_observations(agent, view_rotations)\n",
    "    step_observations.append({key:value[:] for key, value in agent_observations.items()})\n",
    "\n",
    "    # Calculating the distance to the goal after each action\n",
    "    positions = [state.position for state in agent_states]\n",
    "    distances, _ = find_geodesic_distance(positions, unique_position_list[-1])\n",
    "\n",
    "    # Mapping data - needs to be created here whilst sim is still active\n",
    "    #topdown_map = draw_map(agent, scan_id)\n",
    "    gold_path_positions = convert_points_to_topdown(sim.pathfinder, unique_position_list)\n",
    "\n",
    "    if verbose >= 2:\n",
    "        print(f'Actions taken: {past_actions}')\n",
    "        print(f'Final distance to goal: {distances[-1]}')\n",
    "\n",
    "    outputs = {\n",
    "        'inferred_nav_instructions' : inferred_nav_instructions,\n",
    "        'observations' : step_observations,\n",
    "        'step_panoramas' : step_panoramas,\n",
    "        'actions' : past_actions,\n",
    "        'reasoning' : step_reasoning_history,\n",
    "        'agent_states' : agent_states,\n",
    "        'annotation_path' : unique_position_list,\n",
    "        'distances' : distances,\n",
    "        #'topdown_map' : topdown_map, # Comment out this line to reduce file size\n",
    "        'gold_path_positions' : gold_path_positions\n",
    "    }\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7f809ec-b008-484b-8f43-7db3a626ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates through the RxR annotations of a selected scene\n",
    "def iterate_through_scene_annotations(scan_id, rxr_annotations, max_annotations=0, max_actions=10, verbose=0, show_observations=0):   \n",
    "    global cfg\n",
    "    global sim\n",
    "    outputs = dict()\n",
    "\n",
    "    try:\n",
    "        sim.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Instantiate an instance of the scene\n",
    "    update_scan_path(scan_id)\n",
    "    cfg = make_cfg(sim_settings)\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "    agent = sim.get_agent(0)\n",
    "    \n",
    "    annotations = rxr_annotations[scan_id]  # Return list of annotations\n",
    "\n",
    "    # max_annotations == 0 means we use all available annotations for the scan\n",
    "    if max_annotations != 0 and max_annotations < len(annotations):\n",
    "        annotations = random.sample(annotations, max_annotations)\n",
    "\n",
    "    # Run through each annotation and calculate the proximity to end state goal\n",
    "    for n, annotation in enumerate(annotations):\n",
    "        # Get instructions for current annotation data\n",
    "        instruction_id = annotation['instruction_id']\n",
    "        nav_instructions = annotation['instruction']\n",
    "        pose_trace = get_pose_trace(test_pose_dir_path, instruction_id)\n",
    "\n",
    "        print(f'Running through instruction: {instruction_id}. {n+1} of {len(annotations)}')\n",
    "\n",
    "        annotation_outputs = navigate_environment(agent, scan_id, pose_trace, nav_instructions, max_actions=max_actions, verbose=verbose, show_observations=show_observations)\n",
    "\n",
    "        outputs[instruction_id] = annotation_outputs\n",
    "\n",
    "    sim.close()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19d648b3-3843-49a4-8dad-0a09382c60e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through a list of scan_ids and all of their associated instructions\n",
    "# verbose: how much detail to print out\n",
    "#  = 0: show nothing\n",
    "#  = 1: return only progress\n",
    "#  = 2: print progress and LLM decision-making outputs\n",
    "#  = 3: print everything, including VLM descriptions\n",
    "# show_observations: how much observation to show\n",
    "#  = 0: show nothing\n",
    "#  = 1: show starting position and ending position panorama and agent on map\n",
    "#  = 2: show everything, including observations at each step\n",
    "# max_annotations = 0 --> iterate through all available annotations\n",
    "\n",
    "def iterate_through_scans(scan_id_list, rxr_annotations, max_scans=0, max_annotations=0, max_actions=10, verbose=0, show_observations=0):\n",
    "    results = dict()\n",
    "\n",
    "    # max_scans == 0 (or if list is too short) means we use the whole list\n",
    "    if max_scans!= 0 and max_scans < len(scan_id_list):\n",
    "        scan_id_list = random.sample(scan_id_list, max_scans)\n",
    "    \n",
    "    for i, scan_id in enumerate(scan_id_list):\n",
    "        print(f'Running through scan: {scan_id}. {i+1} of {len(scan_id_list)}')\n",
    "        outputs = iterate_through_scene_annotations(scan_id,\n",
    "                                                    rxr_annotations,\n",
    "                                                    max_annotations=max_annotations,\n",
    "                                                    max_actions=max_actions,\n",
    "                                                    verbose=verbose,\n",
    "                                                    show_observations=show_observations)\n",
    "        results[scan_id] = outputs\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae07132-b984-4e99-a351-80adc71a21b2",
   "metadata": {},
   "source": [
    "## 6.2 Navigating with Test Data: Going through Test Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a33d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either running through a new set of trajectories\n",
    "#  or loading in results of previous attempts\n",
    "run_new_trajectories = True\n",
    "model_path = \"/home/wes/Documents/Qwen2-VL-7B_VLN-AWQ-4bit/\"\n",
    "\n",
    "if run_new_trajectories:\n",
    "    # Settings for how many trajectories to run\n",
    "    # Details on where to save outputs\n",
    "    max_scans = 50          # Maximum number of scenes\n",
    "    max_annotations = 3     # Maximum number of annotations / trajectories per scan\n",
    "    max_actions = 60        # Maximum number of actions per experience\n",
    "    verbose = 1\n",
    "\n",
    "    scan_id_list, testing_rxr_annotations, n_records = get_rxr_annotations(testing_annotations_path)\n",
    "\n",
    "    # Filename for results if we are running new test trajectories\n",
    "    custom_message = \"quantized\"   # Use to add a unique, descriptive suffix\n",
    "    current_datetime = datetime.now().date()\n",
    "    results_fname = f\"{current_datetime}_{max_scans}-scans_{max_annotations}-annotations_{max_actions}-actions_{custom_message}\"\n",
    "else:\n",
    "    # Filename to retrieve a set of results calculated earlier\n",
    "    results_fname = '2025-01-10_2-scans_3-annotations_10-actions_initial-test_results.pkl'\n",
    "    results_fpath = output_dir_path + results_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12911750-8f0a-43e6-bbca-d0bf81a254f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_scan_id_list = scan_id_list\n",
    "annotations_list = dict()\n",
    "annotations_list = testing_rxr_annotations\n",
    "#custom_scan_id_list = ['7y3sRwLe3Va']\n",
    "#custom_instruction_id_list = [28]\n",
    "\n",
    "\"\"\"\n",
    "if len(annotations_list) == 0:\n",
    "    for id in custom_scan_id_list:\n",
    "        for annotation in rxr_annotations[id]:\n",
    "            scan_id_annotations = []\n",
    "            \n",
    "            if annotation['instruction_id'] in custom_instruction_id_list:\n",
    "                scan_id_annotations.append(annotation)\n",
    "            \n",
    "            if len(scan_id_annotations) > 0:\n",
    "                annotations_list[id] = scan_id_annotations\n",
    "\"\"\"\n",
    "\n",
    "# Running simulation\n",
    "if run_new_trajectories:\n",
    "    print(f\"Runnnig through new experience.\")\n",
    "    print(f\"Using model: {model_path}\")\n",
    "\n",
    "    processor_vlm_1 = Qwen2VLProcessor.from_pretrained(model_path)\n",
    "    model_vlm_1 = Qwen2VLAWQForConditionalGeneration.from_pretrained(\n",
    "        model_path=model_path,\n",
    "        model_type=\"qwen2_vl\",\n",
    "        use_cache=False,\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "    model_vlm_1.to(\"cuda\")\n",
    "\n",
    "    llm_client = OpenAI()\n",
    "    llm_model = \"gpt-4o\"\n",
    "    \n",
    "    outputs = iterate_through_scans(custom_scan_id_list,\n",
    "                                    annotations_list,\n",
    "                                    max_scans=max_scans,\n",
    "                                    max_annotations=max_annotations,\n",
    "                                    max_actions=max_actions,\n",
    "                                    verbose=verbose,\n",
    "                                    show_observations=0)\n",
    "\n",
    "    save_results(outputs, output_dir_path, results_fname)\n",
    "\n",
    "elif os.path.exists(results_fpath):\n",
    "    outputs = load_results(results_fpath)\n",
    "    print(f\"Existing results loaded: {results_fpath}\")\n",
    "else:\n",
    "    print(f\"New trajectories not created. Existing results could not be found: {results_fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "375d9c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_new_trajectories:\n",
    "    # Cleaning up models\n",
    "    model_vlm_1.to('cpu')\n",
    "    del model_vlm_1\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81032ffb",
   "metadata": {},
   "source": [
    "## Showing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = plot_performance(outputs)\n",
    "df_results.sort_values(by=\"Proportional_Distance\", ascending=False, inplace=True)\n",
    "print(f'Number of trajectories: {df_results.shape[0]}')\n",
    "print(f\"Number of successes: {df_results[df_results['Distance']<=1.0].shape[0]}\")\n",
    "print(df_results.head(20))\n",
    "\n",
    "# Saving output to file\n",
    "if run_new_trajectories: save_results(df_results, output_dir_path, results_fname + '_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156669d7-584d-4fa9-be6d-cd1439f7f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating a particularly interesting scan and annotation\n",
    "#scan_id = 'SN83YJsR3w2'\n",
    "#instruction_id = 0\n",
    "try:\n",
    "    print(df_results.loc[(scan_id, instruction_id)])\n",
    "except:\n",
    "    scan_id = df_results.index[0][0]\n",
    "    instruction_id = df_results.index[0][1]\n",
    "    print(df_results.loc[(scan_id, instruction_id)])\n",
    "\n",
    "print(f'Investigating:')\n",
    "print(f'scan_id: {scan_id}')\n",
    "print(f'instruction_id: {instruction_id}')\n",
    "\n",
    "# Displaying the map and trajectory achieved (vs gold path)\n",
    "show_trajectory(scan_id, instruction_id, outputs[scan_id][instruction_id][\"agent_states\"],\n",
    "                outputs[scan_id][instruction_id][\"annotation_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dda587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking more closely into a single trajectory\n",
    "for annotation in testing_rxr_annotations[scan_id]:\n",
    "    if annotation['instruction_id'] == instruction_id:\n",
    "        instruction = annotation['instruction']\n",
    "print('Instructions:')\n",
    "print(textwrap.fill(instruction, 100) + '\\n')\n",
    "print(outputs[scan_id][instruction_id]['inferred_nav_instructions'])\n",
    "\n",
    "draw_trajectory_observations(outputs[scan_id][instruction_id]['observations'],\n",
    "                             outputs[scan_id][instruction_id]['actions'],\n",
    "                             outputs[scan_id][instruction_id]['reasoning']\n",
    "                             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
